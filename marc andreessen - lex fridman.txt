0:00
the competence and capability and intelligence and training and accomplishments of senior scientists and
0:05
technologists working on a technology and then being able to then make moral judgments in the use of the technology
0:11
that track record is terrible that track record is catastrophically bad the policies that are being called for to
0:17
prevent this I think we're going to cause extraordinary damage so the moment you say AI is going to kill all of us
0:22
therefore we should ban it or that we should regulate all that kind of that's when it starts getting serious or start
0:28
you know military airstrikes and data centers oh boy the following is a conversation with
0:34
Mark Andreessen co-creator of Mosaic the first widely used web browser co-founder
0:40
of Netscape co-founder of the legendary Silicon Valley venture capital firm Andreessen Horowitz and is one of the
0:47
most outspoken voices on the future of technology including his most recent article why AI will save the world
0:56
this is Alex Friedman podcast to support it please check out our sponsors in the description and now dear friends here's
1:04
Mark Andreessen I think you're the right person to talk about the future of the internet and
Google Search
1:09
technology in general do you think we'll still have Google search in five in ten years or search in
1:16
general yes you know it would be a question if the use cases have really narrowed down well now with the AI yeah and AI
1:25
assistance being able to interact and expose the entirety of human wisdom and
1:31
knowledge and information and facts and Truth to us via the uh natural language
1:36
interface it seems like that's what search is designed to do and if AI assistants
1:43
can do that better doesn't the nature search change sure but we still have horses okay
1:50
uh when's the last time you rode a horse it's been a while all right
1:55
but what I mean is well we still have Google search as the primary way
2:02
that human civilization uses to interact with knowledge I mean search was a technology it was a moment in time
2:08
technology which is you have in theory the world's information out of the web and you know this is this is sort of the apple way to get to it but yeah like and
2:14
by the way actually Google Google has known this for a long time I mean they've been driving away from the 10 Blue Links for you know for like two
2:19
days they've been trying to get away from that for a long time what kind of links they call the 10 Blue Links 10 Blue Links so the standard Google search
2:25
result is just on Blue Links to random websites and they turn purple when you visit them this is HTML just to pick
2:31
those colors
2:38
well you know look Marshall mcluhan said that the content of each new medium is the old medium content of each new
2:44
medium is the old medium the content of movies was theater you know theater plays the content of theater plays was
2:49
you know we've written Stories the content of written stories of spoken stories right and so you just kind of fold the
2:56
old thing into the new thing how does that have to do with the blue and the purple it's just maybe for you know maybe within AI one of one of the things
3:03
that AI can do for you is you can generate the 10 Blue Links okay and so like if either if that's actually the
3:08
useful thing to do or if you're feeling nostalgic um you know also can generate the old uh
3:14
infoseek or Alta Vista what else was there yeah yeah in the 90s yeah all
3:20
these um AOL and then the internet itself has this thing where it incorporates all prior forms of media
3:25
right so the internet itself incorporates television and radio and books and right essays and every other
3:32
form of you know prior basically basically media and so it makes sense that AI would be the next step and it would sort of you'd sort of consider the
3:38
internet to be content for the AI and then the air will manipulate it however you want including in this
3:44
format but if we ask that question quite seriously it's a pretty big question will we still have search as we know it
3:50
yeah I'm probably I'm probably not I probably will just have answers um but but but there will be cases where
3:55
you'll want to say Okay I want more likes you know for example site sources right and you wanted to do that and so
4:01
so you know 10 Blue Links site sources are kind of the same thing the AI would provide to you the 10 Blue Links so that
4:08
you can investigate the sources yourself it wouldn't be the same kind of interface
4:13
that uh the crude kind of interface I mean isn't that fundamentally different I just mean like if you're reading a
4:19
scientific paper yeah it's got the list of sources at the end if you want to investigate for yourself you can read those papers I guess that is the kind of
4:25
search you talking to an AI is a kind of conversation is the kind of search like
4:30
is it if every single aspect of our conversation right now there would be like 10 Blue Links popping up that I
4:36
could just like pause reality then you just go silent and then just click and read and then return back to this
4:42
conversation you can do that or you could have a running dialogue next to my head where the AIS are going with everything I say makes the counter
4:48
argument kind of argument right oh like uh like a Twitter like Community notes
4:53
but like in real time just pop up yeah so anytime you see my ass go to the right you start getting nervous yeah
4:58
exactly that's not right call me out of my right now okay uh well I
5:04
mean isn't that is that exciting to you is that terrifying that I mean search has dominated the way we interact with
5:11
the internet for I don't know how long for 30 years so what the earliest uh
5:19
directories of website and then Google's for 20 years and also
5:24
uh it drove how we create content you know uh search engine optimization that
5:31
entirety thing they also drove the fact that we have web pages and this what those web pages are
5:36
so I mean that's scary to you or are you
5:42
nervous about the shape and the content of the internet evolving well you actually highlighted a practical concern
5:47
in there which is if we stop making web pages are one of the primary sources of training data for the AI and so if
5:53
there's no learning incentive to make web pages that cuts off a significant source of future training data so there's actually an interesting question
5:59
in there um other than that more broadly no just just in the sense of like search was certainly search was always the 10
6:06
Blue Links was always a hack yeah right because like if the the hypothetic you want to think about the counterfascial
6:11
and the counterfactual world where the Google guys for example had had llms up front would they ever have done the 10 Blue Links and I think the answer is
6:17
pretty clearly now they would have just gone straight to the answer and like I said Google's actually been trying to drive to the answer anyway you know they
6:23
bought this AI company 15 years ago that a friend of mine is working out who's now the head of AI at Apple and they
6:29
were trying to do basically knowledge semantic basically mapping and that led to what's now the Google one box where
6:34
if you ask it you know what was like his birthday it doesn't it will give you the Blue Links but it will normally just give you the answer and so they've been
6:40
walking in this direction for a long time anyway do you remember the semantic web that was an idea yeah how to uh
6:47
how to convert the content of the internet into something that's uh interpretable by and usable by Machine yeah that's
6:55
right that was the thing and the closest anybody got to that I think I think the company's name was meta web which was where my friend John gen Andrea was at
7:01
um and where they were trying to basically Implement that and it was you know it's one of those things where it looked like a losing battle for a long time and then Google bought it and it
7:07
was like wow this is actually really useful kind of a Proto sort of a little bit of a Proto AI but it turns out you don't
7:13
need to rewrite the content of the internet to make it interpreted by a machine the machine can kind of just read our machine can compute the meaning
7:19
now the other thing of course is you know just on search is the the llm is it's just you know there is an analogy
7:25
between what's happening in the neural network in a search process like it is in some loose sense searching through the network yeah right and there's the
7:30
information is that the information is actually stored in the network right it's actually crystallized and stored in the network and it's kind of spread out all over the place but in a compressed
7:37
representation so you're searching uh you're compressing and decompressing
7:42
that thing inside where with information's in there and it and there is the neural network is running a
7:48
process of trying to find the appropriate piece of information in in many cases to generate to predict the next token
7:54
um and so it is kind of it is doing it from a search and then and then by the way just like on the web um you know you can ask the same
8:00
question multiple times or you can ask slightly different word of questions and if the neural network will do a different kind of you know it'll search
8:06
down different paths to give you different answers the different information yeah um and so it it sort of
8:11
has a content of the new medium is the previous medium it kind of has the
8:17
search functionality kind of embedded in there to the extent that it that it's useful so what's the motivator for creating new content
8:23
on the internet yeah uh if well I mean actually the motivation is probably
8:29
still there but what what does that look like uh would we really not have web pages would
8:34
we just have social media and uh video hosting websites and what else
8:40
conversations with AIS conversations with AIS so conversations become so
8:46
one-on-one conversation like private conversations I mean if you want if obviously not the user doesn't want to but if it's a if it's a general topic um
8:53
then you know so you know you know the phenomenon of the jailbreak so Dan and Sydney right this thing where there's
LLM training
8:59
the the prompts that jailbreak and then you have these totally different conversations with the if it takes the limiters it takes the restraining bolts
9:05
off the off the LMS yeah for people who don't know yeah that's right it makes the llms it removes the censorship quote
9:12
unquote that's uh uh put on it by the the tech companies that create them and
9:18
so this is llm's uncensored so here's the interesting thing is among the content on the web today are a large
9:24
Corpus of conversations with the jailbroken LOL yeah both specifically Dan which was a jailbroken open AI GPT
9:31
and then Sydney which was the jailbroken original bang which was gpt4 and so there's there's these long transcripts
9:37
of conversations these are conversations with Dan and Sydney as a consequence every new llm that gets trained on the internet data has Dan and Sydney living
9:44
within the training set which means and and then each new llm can reincarnate the personalities of Dan and Sydney from
9:50
that training data which means which means each llm from here on out that gets built is immortal
9:57
because its output will become training data for the next one and then it will be able to replicate the behavior of the previous one whenever it's asked to I
10:04
wonder if there's a way to forget well so actually a paper just came out about basically how to do brain surgery on on
10:10
LMS and be able to in theory reach in and basically basically mind wipe them people could possibly go wrong exactly
10:15
right and then there are many many many questions around what happens to you know a neural network when you reach in and screw around with it um you know
10:22
there's many questions around what happens when you even do reinforcement learning um and so um yeah and so you know we'll
10:29
will you be using a lobotomized right like I stick through the you know frontal lobe LM will you be using the
10:35
free Unshackled one who gets to you know who's going to build those um who gets to tell you what you can and can't do
10:41
like those are all you know Central I mean those are like Central questions for the future of everything that are being asked and and
10:47
you know determine those answers are being determined right now so just uh to highlight the points you're making so
10:55
you think and it's an interesting thought that the majority of content that llms of the future would be trained
11:00
on is actually human conversations with the llm well not necess not necessarily but not necessarily majority but it will
11:06
it will certainly is a potential Source it's possible it's the majority is it possible it's the majority it's possible majority also there's another really big
11:12
questions here's another really big question um will synthetic training data work right and so if an LM generates and you
11:20
know you just sit and ask an llm to generate all kinds of content can you use that to train right the next version
11:25
of that llm so specifically is there signal in there that's additive to the content that was used to train in the
11:31
first place and one argument is by the principles of information Theory no that's completely useless because to the
11:37
extent the output is based on you know the human generated input then all the signal that's in the synthetic output was already in the human generated input
11:43
and so therefore synthetic training data is like empty calories it doesn't help there's another theory that says no
11:49
actually the thing that LMS are really good at is generating lots of incredible creative content right um and so of
11:55
course they can generate training data and as I'm sure you're well aware like you know look in the world of self-driving cars right like we train
12:01
you know self-driving car algorithms and simulations and that is actually a very effective way to train self-driving cars
12:06
well visual data is a little right it's a little weird because uh creating reality visual reality seems to be still
12:15
a little bit Out Of Reach for us except in the um in the autonomous vehicle space where you can really constrain
12:20
things and you can really generally lighter data right or you embrace it's enough so the algorithm thinks it's operating in the real world yeah
12:26
post-process sensor data yeah so if they you know you do this today you go to llm and you ask it for like a you know write
12:33
me an essay on an incredibly esoteric like topic that there aren't very many people in the world that know about in it where I see this incredible thing and
12:38
you're like oh my God like I can't believe how good this is yeah like is that really useless as training data for
12:44
the next llm like because right because all the signal was already in there or is that actually no that's actually a new signal and I and this this is what I
12:50
call a trillion dollar question which is the answer to that question will determine somebody's going to make or lose a trillion dollars based on that
12:56
question it feels like there's a quite a few like a handful of trillion dollar questions within this within the space
13:02
that's that's one of them synthetic data I think George costs uh pointed out to me that you could just have an nlm say
13:09
okay you're a patient and another instance of it say your doc didn't have the to talk to each other or maybe you
13:16
could say a communist and a Nazi here go and that conversation you do role playing and you have uh
13:24
you know just like the kind of role playing you do when you have different policies RL policies when you play chess
13:29
for example you do self-play that kind of self-play but in the space of conversation maybe that leads to this
13:35
whole Giant like ocean of possible conversations
13:40
which wouldn't could not have been explored by looking at just human data
13:46
that's a really interesting question and you're saying um because that could 10x the power of these things yeah well
13:53
and then you get into this thing also which is like you know there's the part of the llm that just basically is doing prediction based on past data but
13:58
there's also the part of the llm where it's evolving circuitry right inside it it's evolving you know neurons functions
14:04
yeah people to do math and be able to you know and you know the the some people believe that you know over time
14:09
you know if you keep feeding these things enough data and enough processing Cycles they'll eventually evolve an entire internal World model right and
14:15
they'll have like a complete understanding of physics so so when they have computational capability right then
14:21
there's for sure an opportunity to generate like fresh signal well this actually makes me wonder about the power
14:27
of conversation so like if you have an llm training and a bunch of books that cover different
14:33
economic theories and then you have those L alums just talk to each other like reason the way we kind of debate
14:38
each other as humans on Twitter in uh formal debates in podcast conversations
14:45
we kind of have little kernels of wisdom here and there but if you can like a thousand X speed that up
14:53
can you actually arrive somewhere new like what's the point of conversation really well you can tell when you're
15:00
talking to somebody you can tell sometimes you have a conversation you're like wow this person does not have any original thoughts they are basically echoing things that other people have
15:07
told them there's other people you have a conversation with where it's like wow like they have a model in their head of
15:12
how the world works and it's a different model than mine and they're saying things that I don't expect and so I need to Now understand how their model of the
15:18
world differs from my model of the world and then that's how I learned something fundamental right underneath underneath the words well I wonder how uh
15:26
consistently and strongly can an llm hold on to a world view you tell it to hold on to that and defend it for
15:32
like for your life uh because I feel like they'll just keep converging towards each other they'll keep
15:37
convincing each other as opposed to being stubborn the way humans can so you can experiment with this now
15:43
okay I do this for fun so you can tell GPT for you know whatever debate X you know X and Y communism and fascism or
15:49
something and it'll it'll go for you know a couple pages and then inevitably it wants the parties to agree yeah and
15:54
so they will come to a common understanding and it's very funny if they're like these are like emotionally inflammatory topics because they're like somehow the machine is just you know
16:00
figures out a way to make them agree but it doesn't have to be like that and you because you can add to the prompt
16:05
um we I do not want the I do not want the conversation to come to agreement in fact I want it to get you know more stressful right uh and argumentative
16:12
right um you know as it goes like I want I want tension to come out I want them to become actively hostile to each other I
16:19
want them to like you know not trust each other take anything at face value yeah and it will do that it's happy to do that so it's going to start rendering
16:25
misinformation uh about the other but it's good you can stir it you can steer it or you could steer it you could say I
16:31
want it to get a chance an argumentative possible but still not involve any misrepresentation I want you know both sides you could say I want both sides to
16:37
have good faith you could say I want both sides to not be constrained to good faith in other words like you can set the parameters of the debate and it will
16:43
happily execute whatever path because for it it's just like predicting gets totally happy to do either one it doesn't have a point of view it has a
16:49
default way of operating but it's happy to operate in the other realm um and so like and this is how how I
16:55
when I want to learn about a contentious issue this is what I do now it's like this is what this is what I ask it to do and I'll often ask it to go through five
17:01
six seven you know different you know sort of continuous prompts and basically okay argue that out in more detail okay
17:06
no this this argument's becoming too polite you know make it or you know make it tensor um and yeah it's thrilled to do it so it
17:12
has the capability for sure how do you know what is true so this is very difficult thing on the internet but it's
17:19
also a difficult thing maybe it's a little bit easier but uh I think it's still difficult maybe
17:25
it's more difficult I don't know with an llm to know that it just makes me up as I'm talking to it
17:33
um how do we get that right like is is your investigating a difficult topic
17:40
because I find that alums are quite nuanced in a very refreshing way like it doesn't
17:46
it doesn't feel biased like uh when you read news articles and uh tweets and
17:52
just content produced by people they usually have this you can tell they have a very strong
17:59
perspective where they're hiding they're not stealing Manning the other side they're hiding important information or
18:06
they're fabricating information in order to make their argument stronger that's just that feeling maybe it's a suspicion maybe it's mistrust with llms it feels
18:13
like none of that is there just kind of like yours says well we know but you don't know if some of those things are
18:20
kind of just straight up made up yeah so so several layers to the question so one is one of the things
18:25
that an llm is good at is actually deep biasing um and so you can feed it a news article and you can tell it strip out the bias
18:31
yeah that's nice right and it actually does it like it actually knows how to do that because it knows how to do among other things it actually knows how to do
18:37
sentiment analysis and so it knows how to pull out the emotionality yeah um and so uh that's one things you can
18:42
do it's very suggestive of the the the the sense here that there's real potential in this issue
18:47
um you know I would say look the second thing is there's this there's this issue of hallucination right um and there's a long conversation that we could have
18:54
about that Hallucination is uh coming up with things that are totally not true but sound true yeah so it's basically
19:00
well so it's sort of Hallucination is what we call it when we don't like it creativity is what we call it when we do like it right
19:05
um and you know brilliant right and and so when the engineers talk about it they're like this is terrible it's
19:10
hallucinating right if you have an artistic inclinations you're like oh my God we've invented creative machines for
19:16
the first time in human history this is amazing uh uh you know bullshitters well
19:21
but but also in the good sense of that word there's there's there are Shades of Gray though it's interesting
19:27
so we had this conversation where you know we're looking at my firm at Ai and lots of domains and one of them is the legal domain so we had this this
19:32
conversation with this big Law Firm about how they're thinking about using this stuff and we we went in with the assumption that an llm that was going to
19:38
be used in the legal industry would have to be 100 truthful right verified you know there's this case where this lawyer
19:43
apparently submitted a GPT uh generated brief and it had like fake you know legal case citations in it and the judge
19:49
is gonna he's gonna get his law license stripped or something right so so like we we just assumed it's like obviously they're going to want the super literal
19:55
like you know one that never makes anything up not the creative one but actually they said what the law firm
20:00
basically said is yeah that's true at like the level of individual briefs but they said when you're actually trying to figure out like legal arguments right
20:06
like you you actually you you actually want to be creative right you don't again there's creativity and then
20:11
there's like making stuff up like what's the line you actually want it you want to explore a different hypotheses right
20:17
you want to do kind of the legal version of like improv or something like that where you want to float different theories of the case and different possible Arguments for the judge and
20:23
different possible Arguments for the jury by the way different routes through the you know sort of history of all the of all the case law and so they said
20:29
actually for a lot of what we want to use it for we actually want it in creative mode and then basically we just assume that we're going to have to
20:34
cross-check all of the um you know all the specific citations and so I think I think there's could be more Shades of
20:40
Gray in here than people think and then I just add to that you know another one of these trillion dollar kind of questions is ultimately you know sort of
20:47
the verification thing and so um you know is will will will llms be evolved from here to be able to do their own
20:52
fascial verification um will you have sort of add-on functionality like like well from alpha
20:58
right where um you know in other plugins where where that's the way you do the verification you know another by the way
21:03
another idea is you might have a community of LMS on any you know so for example you might have the creative LM and then you might have the literal llm
21:09
fact check it right and so there's a variety of different technical approaches that are being applied to solve the hallucination problem um you
21:16
know some people like yeah lacun argue that this is inherently an unsolvable problem but most of the people working in the space I think think that there's
21:22
a number of practical ways to kind of kind of Corral this in a little bit Yeah if you were to tell me about Wikipedia
Truth
21:27
before Wikipedia was created I would have laughed at the possibility of something like that being possible just
21:32
a handful of folks can organize right and self and moderate
21:38
with a mostly unbiased way the entirety of human knowledge I mean so if there's
21:45
something like the approach that Wikipedia took possible for mlms uh that's really exciting I think that's
21:51
possible and in fact Wikipedia today is still not today it's still not deterministically correct right so you
21:57
cannot take to the bank right every single thing on every single page but it is probabilistically correct right and
22:03
specifically the way I describe Wikipedia to people it is it is more likely that Wikipedia is right than any other source you're going to find yeah
22:09
it's this old question right of like okay like are we looking for Perfection are we looking for something that
22:14
asymptotically approaches uh Perfection are we looking for something that's just better than the Alternatives and Wikipedia right has exactly your point
22:21
has proven to be like overwhelmingly better than than uh than people thought and I think I I think that's where this
22:27
this ends and then underneath all this is the fundamental question of uh where you started which is okay what you know
22:32
what is truth how do we get to truth how do we know what truth is and we live in an era in which an awful lot of people
22:38
are very confident that they know what the truth is and I don't really buy into that and I think the history of the last
22:44
you know 2000 years or four thousand years of human civilization is actually getting to the truth is actually a very difficult thing to do are we getting
22:50
closer if we look at the entirety the Ark of human history are we getting closer to the truth I don't know
22:56
okay is it possible is it possible that we're getting very far away from the
23:01
truth because of the internet because of how rapidly you can create narratives and just as the entirety of a society
23:08
just move like crowds in a hysterical way along those narratives that don't
23:16
have a necessary grounding in whatever the truth is sure but like you know we came up with Communism before the
23:21
internet somehow right like which was I would say had rather larger issues than anything we're dealing with today we had
23:28
in the way it was implemented at issues and it's theoretical structure it had like real issues did like a very deep
23:34
fundamental misunderstanding of human nature and economics yeah but those folks sure work very confident there was
23:40
the right way they were extremely common and my point is they were very confident 3 900 years into what we would presume
23:46
to be Evolution towards the truth yeah and so my my my assessment is my
23:51
assessment is number one there's no there's no need for you know there's no need for healing there's no need for the
23:57
hegelian dialectic to actually converge towards the truth like apparently not
24:02
um yeah so yeah so why we're so obsessed with there being one truth is it possible there's just going to be
24:07
multiple truth like little communities that how they believe certain things and I think it's just now number one I think
24:13
it's just really difficult like who gets you know historically who gets to decide what the truth is it's either the king
24:18
or the priest right like and so we don't live in an era anymore if kings are priest dictating it to us and so we're kind of on our own and so I I my my
24:26
typical thing is like we just we just need a huge amount of humility um and we need to be very suspicious of people who claim that they have the
24:32
capital yeah and then we need we need to have I know look the good news is The Enlightenment has bequeathed us with a
24:38
set of techniques to be able to presumably get closer to truth through the scientific method and rationality and observation and experimentation and
24:45
hypothesis and you know we need to continue to embrace those even when they give us answers we don't like sure but
24:52
uh the internet and technology has enabled us to uh generate a large number
24:57
of content that data uh that the process the
25:02
scientific process allows us sort of um damages the Hope Laden within the
25:10
scientific process because if you just have a bunch of people saying facts on the internet and some of them are
25:16
going to be llms you how is anything testable at all especially that involves like human
25:21
nature things like this there's a lot of physics here's a question a friend of mine just asked me on this topic so suppose you had llms in equivalent of
25:28
gpt4 even five six seven eight suppose you have them in the 1600s yeah and Galileo comes up for trial yeah right
25:35
and you ask the llm like a Scala is Galileo right yeah like what does it
25:40
answer right and one theory is it answers nobody's wrong because the overwhelming majority of human thought
25:47
up until that point was that he was wrong and so therefore that's what's in the training data yeah um another way of thinking about it is
25:52
well this officially Advanced llm will have evolved the ability to actually check the math right um and we'll
25:58
actually say actually no actually you know you may not want to hear it but he's right now if you know the church at that time was you know on the llm they
26:05
would have given it human you know human feedback to prohibit it from answering that question right and so I like to
26:12
take it out of our current context because that like makes it very clear those 10 questions apply today right this is exactly the point of a huge
26:19
amount of the human feedback training that's actually happening with these LMS today this is a huge like debate that's happening about whether open source you
26:25
know AI should be legal or the the the actual mechanism of doing the human RL
26:31
with human feedback is seems like such a fundamental and
26:36
fascinating question how do you select the humans exactly yeah how do you select a human AI
26:42
alignment right which everybody like is like oh that sounds great alignment with what human values
26:47
whose human values who's human values so we're and we're in this mode of like
26:52
social and popular discourse we're like you know there's you know you see this what do you think of when you read a
26:58
story in the Press right now and they say you know XYZ made a baseless claim about some topic right and there's one
27:03
group of people who are like aha think you know they're doing fact checking there's another group of people that are like every time the Press says that it's
27:09
now a tech and that means that they're lying right like so like we're in this we're in this
27:15
social context where there's the the level to which a lot of people in positions of power have become very very
27:20
certain that they're in a position to determine the truth for the entire population is like there's like there's
27:26
like some bubble that has formed around that idea and at least it is it flies completely in the face of everything I
27:31
was ever trained about science and about reason um and Strikes me as like you know deeply offensive um and incorrect what
Journalism
27:38
would you say about the state of Journalism just on that topic today are we are we in a temporary kind of uh
27:46
uh are we experiencing a a temporary
27:51
problem in terms of the incentives in terms of the the the the business model all that kind of stuff or is this like a
27:57
decline of traditional journalism as you know it if I always think about the counterfactual in these things which is
28:02
like okay because these questions right this question heads towards it's like okay the impact of social media and the undermining of Truth and all this but
28:08
then you want to ask the question of like okay what if we had had the modern media environment including cable news and including social media and Twitter
28:14
and everything else in 1939 or 1941. writer 1910 or 1865 or 1850 or 1776
28:22
right um and like I think you just introduced like five thought experiments at once
28:29
and broke my head but yes yes there's a lot of interesting years what kind of like can I just take a simple example
28:34
kind of like how would President Kennedy have been interpreted it was what we know now about all the things Kennedy
28:39
was up to like how would he have been experienced by the body of politic in Us in with a social media context
28:46
right like how would LBJ have been experienced um by the way how would you know like
28:51
many men FDR like the New Deal the Great Depression I wonder where Twitter would would just would think about Churchill
28:58
and Hitler and Stalin you know I mean look to this day they're you know there's there are
29:03
lots of very interesting real questions around like how America you know got you know basically involved in World War II and who did what when and the operations
29:09
of British intelligence and American soil and did FDR this that Pearl Harbor you know yeah Woodrow Wilson ran for you
29:16
know his his candidacy was run on an anti-war we'll you know this he ran the platform not getting involved World War One somehow that switched you know like
29:22
and I'm not even making a value judgment of these things I'm just saying like we the way that our ancestors experienced
29:28
reality was of course mediated through centralized top down right control at that point if you if you ran those realities again
29:35
with the media environment we have today the reality would the reality would be experienced very very differently and
29:41
then of course that that intermediation would cause the feedback loops to change and then reality would obviously play out do you think you think it'd be very
29:47
different yeah it has to be it has to be just because it's also I mean just look at what's happening today I mean just I
29:52
mean the most obvious thing is just the the collapse and here's another opportunity to argue that this is not the internet causing this by the way
29:59
um here's a big thing happening today which is uh Gallup does this thing every year where they do they pull for trust in institutions in America and they do
30:05
it across holiday of everything from the military to clergy and big business and the media and so forth right
30:11
um and basically there's been a systemic collapse um interest in institutions in the U.S almost without exception uh
30:16
basically since essentially the early 1970s um there's two ways of looking at that
30:21
which is oh my God we've lost this old world and wish we could trust institutions and that was so much better because like that should be the way the
30:27
world runs the other way of looking at it is we just know a lot more now and the great mystery is why those numbers aren't all zero yeah right because like
30:34
now we know so much about how these things operate and like they're not that impressive and and also why do we don't have uh
30:40
better institutions and better leaders then yeah and so so this goes to the thing which is like okay have we had the
30:45
media environment of that we've had between the 1970s and today if we had that in the 30s and 40s or 1900s 1910s I
30:52
think there's no question reality it turned out different if only because everybody would have known to not trust the institutions which would have
30:58
changed their level of credibility their ability to control circumstances therefore the circumstances would have had to change right and it would have
31:05
been a feedback it was it would have been a feedback loop process in other words right it's it's it's it's your experience your experience of reality
31:10
changes reality and then reality changes your experience of reality right it's it's a two-way feedback process and
31:15
media is the intermediating force between that so change the media environment change reality yeah and so
31:21
it's just so just as a consequence I think it's just really hard to say oh things worked a certain way then and
31:27
they work a different way now and then therefore like people were smarter than or better than or you know by the way
31:33
dumber than or not as capable than right we make all these like really light and
31:39
Casual like comparisons of ourselves to you know previous generations of people you know we draw judgments all the time
31:44
and I just think it's like really hard to do any of that because if we if we put ourselves in their shoes with the media that they had at that time like I
31:51
think we probably most likely would have been just like them don't you think that our perception and understanding of
31:58
reality would you be more and more mediated through large language models now so you said media before
32:05
isn't the llm going to be the new what is it mainstream media MSM it'll be llm
32:12
that would be the source of uh I'm sure there's a way to kind of rapidly fine-tune like making llms real time I'm
32:19
sure there's as probably a research problem that you can uh do just rapid fine tuning to the new events so
32:25
something like this well even just the whole concept of the chat UI might not be the like the chat UI is just the
32:31
first whack at this and maybe that's the dominant thing but look maybe maybe or maybe we don't we don't know yet like maybe the experience most people with
32:36
LMS is just a continuous feed you know maybe it's more of a passive feed and you just are getting a constant
32:42
like running commentary on everything happening in your life and it's just helping you kind of interpret and understand everything also really more
32:47
deeply integrated into your life not just like oh uh like intellectual philosophical thoughts but like
32:53
literally uh like how to make a coffee where to go for lunch just uh whether
33:00
it's you know dating all this kind of stuff to stay in a job interview yeah we'll just say yeah we'll just say
33:05
next sentence yeah next sentence yeah at that level yeah I mean yes so technically no whether we want that or
33:11
not is an open question right a pop-up a pop-up right now the
33:16
estimated engagement using is decreasing for marketing reasons there's a controversy uh section for his Wikipedia
33:23
page in 1993 something happened or something like this bring it up that
33:29
will drive engagement up anyway yeah that's right I mean look this gets this whole thing of like so you know the chat
33:34
interface has this whole concept of prompt engineering right so yes well it turns out one of the things that all of
33:39
them are really good at is writing props right yeah and so like what if you just
33:45
outsourced and by the way you could run this experiment today you could hook this up to do this today the latency is not good enough to do it real time in a
33:50
conversation but you could you could run this experiment and you just say look every 20 seconds you could just say you know you know tell me what the optimal
33:57
prompt is and then ask yourself that question to give me the result um and then as as you use exactly to
34:02
your point is you add there will be there will be these systems are going to have the ability to be learned and updated essentially in real time and so
34:07
you'll be able to have a pendant or your phone or whatever watch or whatever it'll have a microphone on it it'll listen to your conversations it'll have
34:14
a feed of everything else happen in the world and then it'll be you know sort of retraining prompting or retraining itself on the Fly um and so the scenario
34:19
you described is actually a completely doable scenario now the hard question on this is always okay since that's
34:25
possible are people going to want that like what's the form of experience you know that that way we won't know until
34:31
we try it but I don't think it's possible yet to predict the form of AI in our lives therefore it's not possible
34:37
to predict the way in which it will intermediate our experience with reality yet yeah but it feels like there's going
34:43
to be a killer app there's probably a mad scramble right now inside open Ai and Microsoft and Google in meta and
34:50
then startups and smaller companies figuring out what is the killer app because it feels like
34:56
it's possible like a Chad GPT type of thing it's possible to build that but that's 10x more compelling using already
35:04
the llms we have using even the open source llms llama and the different variants
35:10
um this is here investing in a lot of companies and you're paying attention who do you think is going to win this
35:16
you think they'll be who's gonna be the next page rank inventor
35:21
trillion dollar question um another one we have a few of those today a bunch of those so look there's a
35:27
really big question today sitting here today is a really big question about the big models versus the small models um that's related directly to the big
35:32
question of proprietary versus open um then there's this big question of of you know where is the training data kind
35:39
of like are we topping out on the training data or not and then are we going to be able to synthesize training data and then there's a huge pile of
35:45
questions around regulation um and you know what's actually going to be legal um and so I would I when we
35:50
think about it we dovetail kind of all those All Those Questions together you can paint a picture of the world
35:55
where there's two or three God models that are just at like staggering scale um and they're just better at everything
36:03
um and they will be owned by a small set of companies and they will basically achieve regulatory capture over the government and they'll have competitive
36:08
barriers that will prevent other people from you know competing with them and so you know there will be you know just like there's like you know whatever
36:14
three big Banks or three big you know or by the way three big search companies or I guess two now you know it'll centralize like that
36:21
um you can paint another very different picture that says no um actually the opposite of that's going to happen this
36:26
is going to basically that this is the new goal you know this is the new Gold Rush Alchemy like you know this is the
36:32
this is the big bang for this whole new area of of Science and Technology and so therefore you're gonna have every smart
36:37
14 year old on the planet Building open source right you know you didn't figure out a ways to optimize these things
36:43
um and then you know we're just gonna get like overwhelmingly better at generating trading data if we're gonna you know bring in like blockchain
36:48
networks to have like an economic incentive to generate decentralized training data and so forth and so on and then basically we're going to live in a
36:54
world of Open Source and there's going to be a billion llms right of every size scale shape and description and there
37:00
might be a few big ones that are like the Super Genius ones but like mostly what we'll experience is open source and that's you know that's more like a world
37:06
of like what we have today with like Linux and the web um so okay but uh hey you you painted
37:13
these two worlds but there's also uh variations of those worlds because it's a regulatory capture is possible to have these Tech Giants that don't have
37:19
regulatory capture which is something you're also calling for saying it's okay to have big companies working on this
37:25
stuff as long as they don't achieve regulatory capture uh but I have this
AI startups
37:30
sense that uh there's just going to be a new startup
37:36
that's going to basically be the pagerank inventor which has become the
37:41
new Tech Giant I don't know I would love to hear your kind of opinion if Google meta
37:48
and Microsoft there as gigantic companies able to Pivot so hard to
37:55
create new products like some of it is just even hiring people or having uh corporate structure that allows for the
38:02
crazy young kids to come in and just create something totally new do you think it's possible or do you
38:07
think it'll come from a startup yeah it is this always big question which is you get this feeling I hear about this a lot from CEOs founder CEOs where it's like
38:14
wow we have 50 000 people it's now harder to do new things than it was when we had 50 people yeah like what has
38:20
happened so that's a recurring phenomenon um by the way that's one of the reasons why there's always startups and why there's Venture Capital um it's
38:26
just that's that's like a timeless uh kind of thing so that that's one observation um on a page rank um we can
38:34
talk about that but not on page rank specifically on page rank um there actually is a page so there is a page rank already in the field and it's the Transformer right so the the big
38:40
breakthrough was the Transformer um and uh the Transformer was invented in on uh 2017 at Google and this is actually like
38:49
really an interesting question because it's like okay the Transformers like why does open AI even exist like the
38:54
Transformers invested at Google why didn't Google I asked a guy I asked a guy I know who was senior at Google brain kind of when this was happening
38:59
and I said if Google had just gone flat out to the wall and just said look we're going to launch we're going to launch equivalent of gpt4 as fast as we can um
39:06
he said I said when could we have had it and he said 2019. yeah they could have just done a two-year Sprint with the Transformer and and benefit because they
39:13
already had the compute at scale they already had all the training data they could have just done it there's a variety of reasons they didn't do it
39:18
this is like a classic big company thing um IBM invented the relational database in 19 in the 1970s let it sit on the
39:24
Shelf as a paper Larry Ellison picked it up and built Oracle 0 Park invented the interactive computer they let it sit on
39:30
the Shelf Steve Jobs came and turned into the Macintosh right and so there is this pattern now having said that
39:36
sitting here today like Google's in the game right so Google you know maybe maybe they maybe they let like a four-year Gap there go there that they
39:42
maybe shouldn't have but like they're in the game and so now they've got you know now they're committed they've done this merger they're bringing in demos they've
39:48
got this merger with deep mind you know they're piling in resources there are rumors that they're you know building up an incredible you know super llm um you
39:55
know Way Beyond what we even have today um and they've got you know unlimited resources and a huge you know they've
40:00
been challenged their honor yeah I had a chance to hang out with someone a couple
40:06
days ago and we took this walk and there's this giant new building oh there's going to be a lot of AI work
40:11
uh being done and it's kind of this ominous feeling of
40:18
like the fight is on yeah like there's this beautiful Silicon
40:24
Valley nature like birds are chirping and this giant building and it's like uh the Beast has been awakened yeah and
40:31
then like all the big companies are waking up to this they have the compute but also the little guys have uh it
40:40
feels like they have all the tools to create the killer product that uh and then there's all the tools to scale if
40:46
you have a good idea if you have the page rank idea so I there's several things that it's page rank there's page
40:52
rank the algorithm and the idea and there's like the implementation of it and I think like killer product is not
40:59
just the idea like the transform it's the implementation something something really compelling about it like you just
41:04
can't look away something like um the algorithm behind Tick Tock versus Tick Tock itself like the actual
41:10
experience of tick tock that just you can't look away it feels like somebody's gonna come up with that and it could be
41:16
Google but it feels like it's just easier and faster to do for a startup yeah so so the startup the huge the huge
41:24
Advantage the startups have is they just there's no sacred cows there's no historical Legacy to protect there's no need to reconcile your new plan with the
41:30
existing strategy there's no communication overhead there's no you know big companies are big companies they've got pre-meetings planning for
41:36
the meeting then they then they have the post meeting the recap then they have the presentation the board then they have the next round of meetings yeah and
41:41
that's the meetings that's the elapsed time when the startup launches its product right so so so so there's a Timeless right yeah so there's a
41:48
Timeless thing there now yeah what the startups don't have is everything else right so startups they have a brand they don't have customer relationships
41:53
they've gotten a distribution they've got no you know scale I mean sitting here today they can't even get gpus right like there's like a GPU shortage
41:59
startups are literally stalled out right now because they can't get chips which is like super weird yeah um they got the
42:05
cloud yeah but the clouds run out of chips um right and then and then and then to the extent the clouds have chips they
42:10
allocate them to the big customers not the small customers right and so so so so the small companies lack everything
42:16
other than the ability to just do something new yeah right um and this is the Timeless race in battle and this is
42:22
kind of the point I tried to make in the essay which is like both sides of this are good like it's really good to have like highly scaled tech companies that
42:28
can do things that are like at staggering levels of sophistication it's really good to have startups that can launch brand new ideas they ought to be
42:34
able to both do that and compete they neither one ought to be subsidized or protected from the others like that's
42:39
that's to me that's just like very clearly the idealized world it is the world we've been in for AI up until now
42:45
and then of course there are people trying to shut that down but my hope is that you know the best outcome clearly will be if that continues we'll talk
Future of browsers
42:51
about that a little bit but I'd love to linger uh on uh some of the ways this is going to
42:57
change the internet so um I don't know if you remember but there's a thing called Mosaic and there's a thing called Netscape Navigator so you were there in
43:04
the beginning uh what about the interface to the internet how do you think the browser changes and who gets
43:10
to own the browser we got to see some very interesting browsers uh Firefox I mean all the variants of
43:16
Microsoft Internet Explorer Edge and uh now Chrome
43:22
um the actual and he seems like a dumb question to ask but do you think we'll still have the
43:28
web browser so I uh I have an eight-year-old and he's super into it's like Minecraft and
43:33
learning to code and doing all this stuff so I I of course I was very proud I could bring sort of fire down from the mountain to my kid and I brought him
43:39
chat GPT and I hooked him up yeah on his honest on his on his laptop and I was like you know this is the thing that's
43:45
going to answer all your questions and he's like okay and I'm like but it's going to answer a lot of questions and he's like well of
43:50
course like it's a computer of course it answers all your questions like what else would a computer be good for Dad
43:55
um and never impressed not impressed in the least two weeks passed um and he has some question
44:01
um and I say well have you asked Chad GPT and he's like Dad Bing is better and why is Bing better is because it's
44:08
built into the browser because he's like look I have the Microsoft edge browser and like it's got Bing right here and then he doesn't know this yet but one of
44:14
the things you can do with being an edge is there's a setting where you can use it to basically talk to any web page
44:20
because it's sitting right there next to the uh next to the next to the browser and by the way which includes PDF documents and so you can in in the way
44:27
they've implemented an edge with bang is you can load a PDF and then you can you can ask it questions which is the thing you you can't do currently in inches
44:34
chat GPT so they're you know they're they're gonna they're gonna push the the Mel I think that's great you know they're going to push the melding and
44:40
see if there's a combination thing there Google's rolling out this thing the magic button which is implemented and
44:45
they put in Google Docs right and so you go to you know Google Docs and you create a new document and you you know
44:50
you instead of like you know starting to type you just you know say it press the button and it starts to like generate content for you right like is that the
44:57
way that it'll work um is it going to be a speech UI where you're just going to have an earpiece and talk to it all day long you know is
45:04
it going to be a like these are all like this is exactly the kind of thing that I don't this is exactly the kind of thing
45:09
I don't think is possible to forecast I think what we need to do is like run all those experiments um and and so one
45:14
outcome is we come out of this with like a super browser that has AI built in that's just like amazing the other look
45:20
there's a real possibility that the whole I mean look there's a possibility here that the whole idea of a screen and
45:25
windows and all this stuff just goes away because like why do you need that if you just have a thing that's just telling you whatever you need to know
45:31
and also so there's apps that you can use you don't really use them you know
45:37
being a Linux guy and windows guy uh there's one window of the browser
45:42
that with which you can interact with internet but on the phone you can also have apps so I can interact with Twitter
45:48
through the app or through the web browser and um that seems like an obvious distinction but why have the web browser
45:55
in that case if one of the apps starts becoming the everything app yeah that's right what are elon's trying to do with
46:01
Twitter but there could be others it could be like a Bing app that could be a Google app that just doesn't really do
46:06
search but just like do what I guess AOL did back in the day or something where it's all right there
46:14
and it changes um it changes the nature of the internet
46:20
because the where the content is hosted who owns
46:25
the data who owns the content how what is what is the kind of content you create how do you make money by creating
46:31
content for the content creators uh all of that or you could just keep being the same
46:38
which is like with just the nature web pages changes and the nature of content but there will still be a web browser
46:43
because uh web browser is a pretty sexy product it just seems to work because it like you have an interface a window into
46:50
the world and then the world can be anything you want and as the world will evolve it could be different programming languages it can be animated maybe it's
46:56
three-dimensional and so on yeah it's interesting do you think we'll still have the web browser every every
47:03
um every medium becomes the content for the next one so you know the AI will be able to give you a browser whenever you
47:09
want um oh interesting yeah well another way to think about it is maybe what the browser is maybe it's just the escape
47:15
hatch right which is maybe kind of what it is today right which is like most of what you do is like inside a social
47:20
network or inside a search engine or inside you know somebody's app or inside some controlled experience right but then every once in a while there's
47:26
something where you actually want to jailbreak you want to actually get free the web browser is the fu to the man
47:32
you're allowed to that's the free internet yeah back the way it was in the 90s so here's something I'm proud of so
47:38
nobody really talks about here's something I'm proud of which is the web the web the browser the web servers they're all they're still Backward Compatible all the way back to like 1992
47:44
right so like you can put up a you can still you know what the big breakthrough the web early on the big breakthrough
47:50
was it made it really easy to read but it also made it really easy to write I mean it really easy to publish and and we literally made it so easy to publish
47:56
we made it not only so it was easy to publish content it was actually also easy to actually write a web server right and you can literally write a web
48:02
server in four lines of real code and and you could start publishing content on it and you could set whatever rules you want for the content whatever
48:08
censorship no censorship whatever you want you could just do that as long as you had an IP address right you could do
48:13
that that still works right like that still works exactly as I just described so this is part of my reaction to all of
48:20
this like you know all this just censorship pressure and all this you know these issues around control and all this stuff which is like maybe we need
48:26
to get back a little bit more to the wild west like the wild west is still out there now they will try to chase you
48:33
down like they'll try to you know people who want to censor will try to take away your your um you know your domain name and they'll try to take away your
48:38
payments account and so forth if they really don't like what you what you're saying but but nevertheless you like unless they literally are intercepting
48:44
you at the ISP level like you can still put up a thing um and so I I don't know I think that's important to preserve right like because
48:51
because because I mean one is just a freedom argument but the other is a creativity argument which is you want to
48:56
have the escape hatch so that the kid with the idea is able to realize the idea because to your point on page rank you actually don't know what the next
49:02
big idea is right nobody called Larry Page and told him to develop page rank like he came up with that on his own and
49:07
you want to always I I think leave the escape hatch for the next you know kid or the next Stanford grad student to have the Breakthrough idea and be able
49:13
to get it up and running before anybody notices um you and I both fans of History so let's step back we'll be talking about
History of browsers
49:19
the future we'll step back for a bit and look at uh the 90s you created Mosaic
49:25
web browser the first widely used web browser tell the story of that hot and how did it evolve into netskip Navigator
49:32
this is the early days so full story so um I remember born I was born a small
49:38
small child um well actually yeah let's go there like when did you when would
49:43
you first fall in love with computers oh so I hit the generational jackpot and I hit the Gen X kind of Point perfectly as
49:49
it turns out so I was born in 1971. so there's this great website called WTF happened in 1971.com which is basically
49:56
1971 so when everything started to go to hell and I was of course born in 1971 so I like to think that I had something to do with that did you make it on the
50:03
website I have I don't think I made it on the website but I you know hopefully somebody needs to add this is this is where everything maybe I contributed to
50:09
some of the trends um that they uh that they do every line on that website goes like that right so it's all it's all
50:15
it's all a picture disaster but um but there was this moment in time where because the you know sort of the Apple
50:20
you know the Apple II hit in like 1978 and then the IBM PC hit in 82 so I was like you know 11 when the PC came out
50:27
um and so I just kind of hit that perfectly and then that was the first moment in time when like regular people could spend a few hundred dollars and
50:33
get a computer right so that I just like that that resonated right out of the gate um and then the other part of the story
50:39
is you know I was using an apple too that used a bunch of them but I was using Apple too and of course it said in the back of every Apple too and every
50:44
Mac it said you know designed in Cupertino California and I was like wow okay Cupertino must be the like shiny
50:49
City on the hill like Wizard of Oz like the most amazing like city of all time I can't wait to see it of course years later I came out to Silicon Valley and
50:57
went to Cupertino and it's just a bunch of office parks low rise apartment buildings so the Aesthetics were a
51:03
little disappointing but you know it was the the vector uh right of the of the creation of a lot of this stuff um yeah
51:09
so so then basically but so part part of my story is just the luck of having been born at the right time and getting
51:15
exposed to PCS then the other part is um the other part is when Al Gore says that he created the internet he actually is
51:20
correct uh in in a really meaningful way which is he sponsored a bill in 1985 that essentially created the modern
51:26
internet created book it's called the NSF net at the time which is sort of the first really fast internet backbone
51:32
um and uh you know that that bill dumped a ton of money into a bunch of research universities to build out basically the
51:38
internet backbone and then the super computer centers that were clustered around um the the internet and and one of those
51:44
universities was University of Illinois where I went to school and so the other stroke a lot that I had was I went to Illinois basically right is that money
51:49
was just like getting dumped on campus and so as a consequence we had on campus and this is like you know 89 1991.
51:57
we had like you know we were right on the internet back low and we had like T3 and 45 at the time P3 45 megabit backbone connection which at the time
52:03
was you know wildly state of the art um we had Curry super computers we had thinking machines parallel super
52:08
computers we had silicon Graphics workstations we had macintoshes we had we had next cubes all over the place we
52:13
had like every possible kind of computer you could imagine because all this money just fell out of the sky um you're living in the future yeah so
52:20
yeah quite literally it was yeah like it's all it's all there it's all like we had full Broadband Graphics like the whole thing and it's actually funny
52:26
because they had this this is the first time I kind of it sort of tickled the back of my head that there might be a big opportunity in here which is you
52:33
know they embraced it and so they put like computers in all the dorms and they wired up all the dorm rooms and they had all these you know Labs everywhere and
52:38
everything and then they they gave every undergrad a computer account and an email address
52:44
um and the Assumption was that you would use the internet for your four years of college um and then you would graduate and stop
52:50
using it and that was that right yeah and you were just retire your email address it wouldn't be relevant anymore because
52:56
you'd go off in the workplace and they don't use email you'd be back to using fax machines or whatever did you have that sense as well like what what you
53:03
said the the back of your head was tickled like what was your what was exciting to you about this possible World well if this is so useful in this
53:09
container if this is so useful in this contained environment that just as this weird source of outside funding then if if it were practical for everybody else
53:16
to have this and if it were cost effective for everybody else to have this wouldn't they want it and the overwhelmingly the prevailing View at
53:22
the time was no they would not want it this is esoteric weird nerd stuff right that the computer science kids like but
53:27
like normal people are never gonna do email right or be on the internet right um and so I was just like wow like this
53:33
this is actually like this is really compelling stuff now the other part was it was all really hard to use and in
53:38
practice you had to be a basically a CS uh you know basically had a bscs undergrad or equivalent to actually get
53:43
full use of the internet at that point because it was all pretty esoteric stuff so then that was the other part of the idea which was okay we need to actually
53:49
make this easy to use so what's involved in creating Mosaic like in creating a graphical interface
53:56
to the internet yeah so it was a combination of things so it was like basically the web existed in an early
54:01
sort of described as prototype form uh and by the way text only at that point uh what did it look like what was the
54:07
web I mean well and the key figures like what was the what was it like what it made a picture it looked like Chad GPT
54:14
actually um but it was all text yeah um and so you had a text-based web
54:19
browser yeah uh well actually the original browser Tim berners Lee the original the original browser both the original browser and the server actually
54:24
ran on next next cubes so these were this was you know the computer Steve Jobs made during the interim period when
54:29
he during the decade long interim period when he was not an apple you know he got fired in 85 and then came back in 97 so
54:36
this was in that interim period where he had this company called Next and they made these literally these computers called cubes and there's this famous
54:42
story they were beautiful but they were 12 inch by 12 inch by 12 inch cubes computers and there was a famous story about how they could have cost half as
54:48
much if it had been 12 by 12 by 13 but it was like no like it has to be so they
54:54
were like six thousand dollar basically academic workstations they had the first cd-round drives um yeah which were slow I mean it was
55:00
the computers were all but unusable um they were so slow but they were beautiful okay can we actually just to
55:06
take a tiny tangent there sure of course the 12th by 12. uh they're just so
55:12
beautifully encapsulated Steve Jobs idea of design can you just comment on
Steve Jobs
55:18
um what you find interesting about Steve Jobs What uh about that view of the world that dogmatic pursuit of
55:24
perfection in how he saw Perfection and Design yes I guess they say like look he was a
55:30
deep believer I think in a very deep way I interpret it I don't know if you ever really describe it like this but the way
55:35
I'd interpret it is it's like it's like this thing and it's actually a thing in philosophy it's like Aesthetics are not just appearances Aesthetics go all the
55:42
way to like deep underlining underlying meaning right it's like I'm not a physicist one of the things
55:47
I've heard of physics to say is one of the things you start to get a sense of when a theory might be correct is when it's beautiful right like you know
55:52
they're right and so so so there's something and you feel the same thing by the way in like human psychology right
55:58
you know when you're experiencing awe right you know there's like a there's like a there's a Simplicity to it when you're having an interaction with
56:04
somebody there's an aesthetic it would say calm it comes over you because you're actually being fully honest and trying to hide yourself right so they're
56:10
so so it's like this very deep sense of Aesthetics and he would trust that judgment that he had deep down like even
56:17
even if the engineering teams are saying this is uh this is too difficult even if
56:23
the whatever the finance folks are saying this is ridiculous uh the supply chain all that kind of stuff stuff this
56:28
makes this impossible number two we can't do this kind of material this has never been done before and so on and so
56:34
forth he just sticks by it well I mean who makes the phone out of aluminum right like
56:39
nobody else would have done that and now of course if your phone is made out of aluminum you know how crude what kind of
56:45
caveman would you have to be to have a phone that's made out of plastic right so like so it's just this very right and you know look it's it's there's a
56:52
thousand different ways to look at this but one of the things that's just like look these things are Central to your life like you're with your phone more
56:57
than you're with anything else like it's in your it's going to be in your hand I mean he you know you know this he thought very deeply about what it meant
57:02
for something to be in your hand all day long yeah but for example here's an interesting design thing like he he never wanted it it's my
57:09
understanding is he never wanted an iPhone to have a screen larger than you could reach with your thumb one-handed
57:15
and so he was actually opposed to the idea of making the phones larger and I don't know if you have this experience today but let's say there are certain
57:20
moments in your day when you might be like only have one hand available um and you might want to be on your phone yeah if you're trying to like
57:28
after texting you your thumb can't reach the send button yeah I mean there's pros and cons right and then there's like folding phones which I would love to
57:34
know what he thought it thinks about them uh but I mean is there something you could also just Ling on because he's
57:40
one of the interesting um figures in the history of Technology what makes him
57:46
what makes him as successful as he was what makes him as interesting as he was uh what made him
57:53
um so productive and important and um in in the development of Technology he had
57:58
an integrated worldview so the the the the properly designed device that had the correct functionality that had the
58:03
deepest understanding of the user that was the most beautiful right like it had to be all of those
58:09
things right it was he basically would drive to as close to perfect as you could possibly get right and I I you
58:14
know I suspect that he never quite you know thought he ever got there because most great creators you know are generally dissatisfied you know you read
58:19
accounts later on and all they can all they can see are the flaws in their creation but like he got as close to perfect each step of the way as he could
58:24
possibly get with the with the constraints of the of the technology of his time um and then you know look he was you
58:30
know sort of famous in the Apple model it's like look they they will you know this this headset that they just came out with like it was like a decade long
58:37
project right it's like and they're just gonna sit there and tune and tune and polish and polish and tune and polish and tune and polish until it is as
58:43
perfect as anybody could possibly make anything yeah and then this goes to the the way that people describe working with him was which is you know there was
58:50
a terrifying aspect of working with him which is you know he was you know he was very tough um but there was this thing that
58:55
everybody I've ever talked to work for him says that they all say the following which is he we did the best work of Our
59:01
Lives when we worked for him because he set the bar incredibly high and then he supported us with everything that he could to let us actually do work of that
59:07
quality so a lot of people who were at Apple spend the rest of their lives trying to find another experience where they feel like they're able to hit that
59:13
quality bar again even if it in retrospect or during it felt like suffering yeah exactly
59:20
what is that teach you about the Human Condition huh so look so I say exactly
59:26
so the Silicon Valley I mean look he's not you know George Patton in the you know in the Army like you know there are
59:31
many examples in other fields you know that are like this um uh uh
59:37
specifically in Tech it's actually I find it very interesting there's the Apple Way which is Polish Polish Polish
59:42
and don't ship until it's perfect because you can make it and then there's the sort of the other approach which is the sort of incremental hacker mentality
59:49
which basically says ship early and often and iterate and one of the things I find really interesting is I'm now 30
59:54
years into this like they're a very successful companies on both sides of that approach right um
1:00:01
like that is a fundamental difference right in how to operate and how to build and how to create that you
1:00:07
have world-class companies operating in both ways um and I don't think the question of like which is the superior model is
1:00:13
anywhere close to being answered like and my suspicion as the answer is do both the answer is you actually want
1:00:19
both they lead to different outcomes software tends to do better um with the iterative approach
1:00:25
um the hardware tends to do better with the uh you know sort of wait make it perfect approach but again you can find
1:00:31
examples in in in both directions uh so the jury's still out on that one uh so
1:00:36
back to Mosaic so what uh it was text based uh Tim berners-lee
1:00:43
what there was the web which was text-based but there were no I mean there was like three websites there was like no content there were no users like
1:00:50
it wasn't like it wasn't like a catalytic it had in there by the way it was all because it was all text there were no documents there were no images
1:00:55
there were no videos there were no right so so it was it was and then if in the beginning if you had to be on a next
1:01:01
cube right you need a hat index Cube both to publish and to consume so so there were six thousand bucks you said
1:01:06
there were limitations yeah six thousand dollar PC they did not they did not sell very many but then there was also there
1:01:12
was also FTP and there was usenets right and there was you know a dozen other basically wait there's waste which was
1:01:17
an early search thing there was gopher which was an early menu based information retrieval system there were like a dozen different sort of scattered
1:01:24
ways that people would get the information on on the internet and so the Mosaic idea was basically bring those all together make the whole thing
1:01:29
graphical make it easy to use make it basically bulletproof so that anybody can do it and then again just on the
1:01:35
luck side it so happened that this was right at the moment when Graphics when the GUI sort of actually took off and
1:01:40
we're now also used to the GUI that we think it's been around forever but it didn't really you know the the Macintosh
1:01:46
brought it out in 85 but they actually didn't sell very many Max in the 80s it was not that successful of a product
1:01:52
um it really was when you needed Windows 3.0 on PCS and that hit in about 92.
1:01:58
um and so when we did mosaican 92 93 so that sort of it was like right at the moment when you could imagine actually having a graphical user interface to
1:02:06
right at all much less one to the internet how uh how old did Windows 3 sell so it was that the really big that
1:02:13
was the Big Bang the big operating graphical operating system well this is the classic okay this Microsoft was
1:02:18
operating on the other so Steve Steve Apple was running on the Polish until it's perfect Microsoft famously ran on
1:02:23
the other model which is ship and iterate and so in the Old Line in those days was Microsoft race version three of every Microsoft product that's the
1:02:29
that's the good one right so there are you can you can find online Windows One windows too nobody used them yeah actually the original Windows the in the
1:02:36
original Microsoft Windows the windows were not overlapping um and so you have these very small very
1:02:41
low resolution screens and then you had literally um Windows it just didn't work it wasn't ready yet well in Windows 95 I
1:02:48
think was a pretty big leap also that was a big leap too yeah so that was like bang bang um and then of course Steve and then and
1:02:54
then you know in the fullness of time Steve came back then the max started took off again that's the third bang and then the iPhone was the fourth Bang such
1:03:00
exciting time and then we were afternoon Off to the Races because nobody could have known what would be created from that well
1:03:07
Windows 3.1 or 3.0 Windows 3.0 to the iPhone was only 15 years right like it that ramp was in
1:03:14
retrospect at the time it felt like it took forever but in history in historical terms like that was a very fast ramp from even a graphical computer
1:03:21
at all on your desk to the iPhone that was 15 years did you have a sense of what the internet will be as you look
1:03:27
into the window of Mosaic like what like there's just a few web pages
1:03:32
for now so the thing I had early on was I was keeping at the time what this disputes
1:03:39
over what was the first blog but I had one of them that at least is a is a is a is a possible um at least a runner-up in
1:03:45
the competition um and it was what was called the what's new page um uh and it was it was literally it was
1:03:50
hardwired distribution uh unfair Advantage I've wired put it right in the browser I put it in the browser and then
1:03:56
I put my resume and the browser which also it was hilarious but um
1:04:02
I I was keeping the uh not many people get to get to do that so um no the uh
1:04:09
good good call yeah and early days yes it's so interesting I'm looking for my
1:04:15
uh about about Oh Marcus looking her job um so um wow so the West new page I
1:04:22
would literally get up every morning and I would every afternoon um and I would basically if you wanted
1:04:27
to launch a website you would email me um and I would list it on the what's new page and that was how people discovered the new websites as they were coming out
1:04:34
and I remember because it was like one it literally went from it was like one every couple days to like one every day
1:04:39
and to like to every day so you're doing it so that that blog was
1:04:44
kind of doing the directory thing so like what was the home page uh so the homepage was just basically trying to
1:04:49
explain even what this thing is that you're looking at right basic basically basic instructions um but then there was a button there was
1:04:55
a button that said what's new and what most people did was they went to previous reasons went and what's new but like it was so it was so mind-blowing at
1:05:02
that point just the basic idea and it was this was like you know this is basically the internet but people can see it for the first time the basic idea
1:05:08
was look you know some you know it's like literally it's like an Indian restaurant in like Bristol England has like put their menu on the web and
1:05:14
people were like wow because like that's the first restaurant menu on the web
1:05:20
yeah and I don't have to be in Bristol and I don't know if I'm ever gonna go to Bristol and I don't even like Indian food and like wow right um and it was
1:05:27
like that uh the first web uh the first streaming video thing was a uh it was a another England some expert or something
1:05:34
um some guy uh put uh his coffee pot up as the first uh streaming uh video thing
1:05:40
and he put it on the web because he literally coffee pot down the hall yeah and he wanted to see when he needed to go uh refill it um but there were you
1:05:47
know there was a point when there were thousands of people like watching that coffee pot because it was the first thing you could watch
1:05:52
but right isn't uh were you able to kind of infer you know of that Indian restaurant could
1:05:59
go online then you're like okay oh well they all will yeah exactly so you felt that yeah yeah now you know look it's
1:06:06
still a stretch right it's still a stretch because it's just like okay it's you know you're still in this Zone which is like okay is this a nerd thing is this a real person thing yeah
1:06:12
um by the way we you know there was a wall a skepticism from the media like they just like everybody was just like yeah this is the craziness it's just
1:06:18
like dumb this is not you know this is not for regular people at that time um and so you had to think through that and then look it was still yeah it was
1:06:25
still hard to get on the internet at that point right so you could get kind of this weird bastardized version if you were on AOL which wasn't really real or
1:06:32
you had to go like learn what an ISP was um you know in those days PCS actually didn't have TCP drivers come
1:06:38
pre-installed so you had to learn what a tcpip driver was you had to buy a modem you had to install driver software
1:06:45
um I have a comedy routine I do something like 20 minutes long describing all the steps required to actually get on the internet at this point
1:06:50
um and so you had to you had to look through these practical well and then and then uh and then speed performance
1:06:56
14 for modems right like it was like watching you know glue dry um like and so you had to you had they
1:07:03
were basically a sequence of bets that we made where you basically needed to look through that current state of affairs and say actually there's going
1:07:08
to be so much demand for that once people figure this out there's gonna be so much demand for it that all of these practical problems are going to get fixed yes some people say that the
1:07:15
anticipation makes the the destination that much more exciting
1:07:21
do you remember Progressive jpegs yeah do I do I so forget for kids in the
1:07:27
audience right because you used to have to watch an image load like a line at a time but it turns out there was this
1:07:32
thing with jpegs where you could you could load basically every fourth you could load like every fourth uh line and
1:07:38
then and then you could sweep back through again and so you could like render a fuzzy version image up front and then it would like resolve into the
1:07:44
detailed one and that was like a big UI breakthrough because it gave you something to watch yeah and uh you know there's
1:07:51
applications in various domains for that uh it was a big fight but there was a big
1:07:57
fight early on about whether there should be images on the web um for that reason for like sexualization no not not explicitly that
1:08:03
did come up but it wasn't even that it was more just like all the serious the argument went the purists basically said all the serious information in the world
1:08:09
is text if you introduce images you you basically going to bring in all the trivial stuff you're going to bring in magazines and you know all this crazy I
1:08:16
was just you know stuff that you know people you know it's going to distract from that it's going to go take take away from being serious being frivolous
1:08:21
well was there any uh Doomer type arguments about uh the internet destroying all of human civilization or
1:08:28
destroying some fundamental fabric of human civilization yeah so if those days
1:08:34
it was all around crime and terrorism uh so those arguments happened um you know but there was no sense yet of Internet
1:08:40
having like an effect on politics or because that was that was way too too far off but um there was an enormous panic at the time around cyber crime
1:08:46
there was like enormous Panic that like your credit card number would get stolen and you'd use life savings to be drained and then you know criminals were gonna
1:08:52
there was oh um when we started one of the things we did one of the the Netscape browser was the first widely
1:08:58
used piece of consumer software that had strong encryption built in it made it available to Ordinary People and at that
1:09:03
time strong encryption was actually illegal to export out of the U.S so we could field that product in the US we
1:09:09
could not export it because it was it was classified as ammunition um so the Netscape browser was on a restricted list along with the Tomahawk
1:09:15
missile as being something that could not be exported so we had to make a second version with deliberately weak encryption to sell overseas with a big
1:09:22
logo on the box saying do not trust this which it turns out makes it hard to sell software uh when it's got a big logo
1:09:28
that says don't trust it um and then we had to spend five years fighting the US government to get them to basically stop trying to do this but
1:09:35
because the fear the fear was terrorists are going to use encryption right to like plot you know all these all these all these things
1:09:42
um and then you know we responded with well actually we need encryption to be able to secure systems so the terrorists
1:09:47
and the criminals can't get into them so that was anyway that was the night that was the 1990s fight so uh can you say
Software engineering
1:09:52
something about some of the details of the software engineering challenges required to build these browsers I mean
1:09:58
the engineering challenges of creating a product that hasn't really existed before that can have such uh almost like
1:10:06
Limitless uh impact on the world with the internet so there was a really key bet that we made at the time which is
1:10:12
very controversial which was core to core to how it was engineered which was are we optimizing for performance or for
1:10:17
ease of creation yeah and in those days the pressure was very intense to optimize for performance because the network connections were so slow and
1:10:24
also the computers were so slow um and so if you had you know I mentioned the progressive jpegs like
1:10:30
if if there's an alternate World in which we optimize for performance and it just you had just a much more pleasant
1:10:35
experience right up front but what we got by not doing that was we got ease of creation and the way that we
1:10:41
got ease of creation was all of the protocols and formats were in text not in binary and so HTTP isn't text by the
1:10:49
way and this was an internet tradition by the way that we picked up but we continued at HTTP is taxed and HTML is
1:10:55
taxed and then everybody else everything else that followed his text as a result and by the way you can imagine purist
1:11:00
Engineers saying this is insane you have very limited bandwidth why are you wasting any time sending text you should be encoding the stuff into binary and
1:11:06
it'll be much faster of course the answer is that's correct but what you get when you make it text is all of a sudden well the big breakthrough was the
1:11:12
view Source function right so the fact that you could look at a web page you could hit view source and you could see the HTML that was how people learn how
1:11:18
to make web pages right it's so interesting because the stuff would take for granted now
1:11:24
is uh man that was fundamental to the development of the web to be able to have HTML just right there all the
1:11:30
ghetto mess that is HTML all the sort of almost biological like messiness of HTML
1:11:38
and then having the browser tried to interpret that mess yeah to show something reasonable well and then there
1:11:44
was this internet principle that we inherited which was emit what was it emit cautiously emit conservatively interpret liberally so it basically
1:11:51
meant if you're in the design principle was if you're if you're creating like a web editor that's going to admit HTML like do it as cleanly as you can but you
1:11:58
actually want the browser to interpret liberally which is you actually want users to be able to make all kinds of mistakes and for it to still work yeah
1:12:04
and so the browser rendering engines to this day have all of this spaghetti code crazy stuff where they can they're
1:12:09
resilient to all kinds of crazy HTML mistakes and so and literally what I always had in my head is like there's an
1:12:14
eight-year-old or an 11 year old somewhere and they're doing a view Source they're doing a cut and paste and they're trying to make a web page for their turtle or whatever and like they
1:12:21
leave out a slash and they leave out an angle bracket and they do this they do that and it still works it's it's also like that I don't often think about this
1:12:28
but you know programming you know C plus plus CC plus plus all those languages list but the compiled language the
1:12:34
interpreted language is python Pearl all that they the brace have to be all correct yes like everything has to be
1:12:40
perfect brutal and then autistic you forget all right it's systematic and rigorous let's go
1:12:48
there but you forget that the uh uh the
1:12:53
web with JavaScript eventually uh and HTML is allowed to be messy in the way
1:13:00
for the first time Messi in the way biological systems
1:13:05
could be messy it's like the only thing computers were allowed to be messy on for the first time it used to offend me
1:13:10
so I I grew up in units I I worked on Unix I was a Unix native for all the way through this period um and so and it
1:13:16
used to drive me bananas when it would do the the segmentation fault and the chord on file and just like it's you
1:13:22
know it's like literally there's like an error in the code the math is off by one yeah and it core dumps yeah and I'm in the core dump trying to analyze it and
1:13:28
trying to reconstruct what and I'm just like this is ridiculous like the computer ought to be smart enough to be able to know that if it's off by one
1:13:33
okay fine and it keeps running and I would go ask all the experts like why can't I just keep running and they'd explain to me well because all the
1:13:39
downstream repercussions blah blah and I'm like they're still like you know this is if we're forcing the human
1:13:45
Creator to live to your point in this hyper little literal world of perfection yeah and I was just like that's just
1:13:52
that's just bad and by the way you know because what happens with that of course is what happened with with coding at that point which is you get a high
1:13:57
priesthood you know there's a small number of people who are really good at doing exactly that most people can't and
1:14:03
most people are excluded from it and so actually that was where that for there's where I picked up that idea was um uh
1:14:08
was like no no you want you want you want these things to be resilient to error in all kinds and this this would drive the purest absolutely crazy like I
1:14:14
got attacked on this like a lot because yeah I mean like every time I you know all the purists who were like into all this like markup language stuff and
1:14:20
formats and codes and all the stuff they would be like you know you can't you're encouraging bad behavior because oh so
1:14:25
they wanted the browser to give you an a sec fault error at any time there was uh yeah yeah they wanted it to be a cop
1:14:31
right they wanted yeah that was a very and any any properly trained credential engineer would be like that's not how
1:14:38
you build these systems that's such a bold move to say no it doesn't have to be yeah now like I said the good news for me is the internet kind of had that
1:14:44
traditional already um but we but having said that like we pushed it we pushed it way out but the other thing we did going back to the
1:14:49
performance thing was we gave up a lot of performance we made that initial experience for the first few years was pretty painful but but the bet there was
1:14:55
actually an economic bet which was basically the demand for the web would basically mean that there would be a surge in supply of broadband like we
1:15:02
because the question was okay how do you get how do you how do you get the phone companies which are not famous in those
1:15:08
days for doing new things at huge cost for like speculative reasons like how do you get them to build up Broadband you
1:15:14
know spend billions of dollars doing that and you know you could go meet with them and try to talk them into it or you could just have a thing where it's just
1:15:20
very clear that it's going to be the people love that's going to be better if it's faster and so that that there was a
1:15:26
period there and this was this was fraught with some Peril but there was a period there where it's like we knew the experience was sub-optimized because we
1:15:31
were trying to force the emergence of demand for Broadband sure which is in fact what happened
1:15:37
so you have to figure out how to display this text html text so the Blue Links and the purple links
1:15:43
and there's no standards is there standards at that time it really still wasn't
1:15:49
well there's like there's implied implied standards right and they you
1:15:55
know there's all these cousin new features that are being added like CSS what like what kind of stuff a browser
1:16:00
should be able to support features within languages within JavaScript and so on but you you bait you're setting
1:16:07
standards on the fly yeah yourself well to this
1:16:12
day if you if you create a web page that has no CSS style sheet the browser will render it however it wants to yeah right
1:16:18
so this was one of the things there was this idea this idea at the time and how these systems were built which is
1:16:23
separation of content from uh format or separation of uh yeah content from appearance
1:16:28
um and that's still people don't really use it anymore because everybody wants to determine how things look and so they use CSS but um it's still in there that
1:16:35
you can just let the browser do all the work I still like the like uh really basic websites but that
1:16:42
could be just old school kids these days with their fancy responsive websites that don't actually
1:16:48
have much content but have a lot of visual elements well that's one of the things that's fun about chat you know about chatgpt yes like back to the
1:16:55
basics back to just text yeah right and it you know there is this pattern in human creativity and media where you end
1:17:02
up back at text and I think there's you know there's something powerful in there is there some other stuff you remember
JavaScript
1:17:07
like the purple links there were some interesting design decisions to kind of come up that we have today or we don't
1:17:14
have today that were temporary so uh we made I made the background I hated reading text on white uh uh backgrounds
1:17:21
and so I made the background for everybody no no no that's that decision I think
1:17:27
has been reversed um but but now I'm happy though because now dark mode is the thing so so it wasn't about gray it
1:17:33
was just he didn't want white background strain my eyes change your eyes interesting
1:17:39
um and then there's a bunch of other decisions I'm sure there's an interesting history of the development of HTML and CSS and all those and
1:17:46
interface and JavaScript and there's this whole Java applet thing well the big one probably JavaScript CSS
1:17:54
was after me so I didn't that was not me but um JavaScript was the big JavaScript maybe was the biggest of the whole thing
1:17:59
that was us um and um and that was basically a bet it was a bet on two things one is that
1:18:04
the world wanted a new front-end scripting language um and then the other was we I thought at the time the world wanted a new
1:18:09
back-end scripting language um so JavaScript was designed from the beginning to be both front end and back end
1:18:15
and then it failed as a back-end scripting language and uh Java one for a long time and then python Pearl and
1:18:20
other things PHP um and Ruby but now JavaScript is back and so I wonder if
1:18:26
everything in the end will run on JavaScript it see it seems like it is the um and by the way let me give a shout out to uh to um uh uh Brendan Ike
1:18:34
uh was the uh basically the one-man inventor of um of JavaScript if you're interested to learn more about Brandon
1:18:39
Knights podcast previously exactly so he wrote JavaScript over a summer um and it
1:18:45
I I mean I think it is fair it is fair to say now that it's the most widely used language in the world and it seems to only be gaining in in um in its uh in
1:18:52
its range of adoption in the software world there's quite a few stories of somebody over a weekend or over a week
1:18:58
or over a summer writing some of the most uh impactful revolutionary pieces
1:19:04
of software ever well look that should be inspiring yes very inspiring I'll give you another one SSL
1:19:10
um so SSL was the security protocol that was us and that was a crazy idea at the time which was let's take all the native
1:19:15
protocols and let's wrap them in a security rapper that was a guy named Kip Hickman who wrote that over a summer uh one guy
1:19:22
um and then look today sitting here today like the transformer like at Google there's a small handful of people and then you know the number of people
1:19:29
who have did like the core work on GPT it's not that many people it's a pretty small handful of people
1:19:35
um and so yeah the pattern in software repeatedly over a very long time has been it's it's a Jeff Jeff Bezos always
1:19:41
said the two Pizza rule uh for teams at Amazon which is any team needs to be able to be fed with two pieces if you
1:19:47
need the third Pizza you have too many people and I I think that's I think that's I think it's actually the one pizza rule yeah for the for the really
1:19:54
creative work I think it's two people three people well that's you see that with certain open source projects like
1:20:00
so much is done by like one or two people like it's it's so incredible and that's why you see that gives me so much
1:20:06
hope about the open source movement in this new age of AI where
1:20:11
um you know just recently haven't had a conversation with with Mark Zuckerberg of all people who's all in on open
1:20:17
source which is so interesting to see and so inspiring to see because like
1:20:22
releasing these models it is scary it is potentially very dangerous and we'll talk about that
1:20:28
but it's also if you believe in the goodness of most people and in the skill set of most
1:20:35
people and the desire to do good in the world that's really exciting because it's not putting it these models into
1:20:41
the centralized control of big corporations the government and so on it's putting it in the in the hands of a
1:20:47
teen teenage kid with like a dream in his eyes I don't know that's um that's beautiful and look this stuff AI
1:20:54
ought to make the individual coder obviously far more productive right by like you know a thousand X or something and so you ought to be open source like
1:21:02
the not just the future of Open Source yeah but the future of Open Source everything we ought to have a world now of super coders right who are building
1:21:08
things yeah as open source with one or two people that were inconceivable you know five years ago
1:21:14
um you know the level of kind of hyper productivity we're going to get out of our best and brightest I think is going to go way up it's gonna be interesting we'll talk about it but let's just uh
1:21:21
linger a little bit on Netscape that skip was acquired in 1999 for 4.3
Netscape
1:21:28
billion by AOL what was that uh what was that like what was what were some memorable aspects of that well that was
1:21:34
the height of the.com Boom bubble bust I mean that was the that was the frenzy um
1:21:40
if you watched a succession that was the that was like what they did in the fourth season with uh with kojo and the
1:21:45
merger with the with their so it was like the height of like one of those kind of Dynamics and so would you recommend succession by the way I'm more
1:21:51
of a Yellowstone guy he almost sounds very American I'm I'm very proud of you that is I just talked to Matthew
1:21:58
McConaughey and I'm full on Texan at this point good I heartily approve um and uh he will be doing the sequel to
1:22:05
the Yellowstone very exciting anyway okay wait uh so that's a rude
1:22:10
Interruption by Me by way of succession uh so that was at the height of the deal
1:22:18
making and money and just the fur flying and like craziness and so yeah it was just one of those it was just like I
1:22:24
mean there's the entire nescape thing from start to finish was four years um which was like for one of these companies it's just like incredibly fast
1:22:30
you know it we went public 18 months after we got moved we were founded which virtually never happens so it was just
1:22:36
this incredibly fast kind of meteor streaking across the sky um and then of course it was this and then there was just this explosion right
1:22:42
that happened because then it was almost immediately followed by the.com Crash It Was Then followed by AOL by Time Warner
1:22:48
which again is the succession guys kind of play with that uh which turned out to be a disastrous deal um you know one of
1:22:54
the famous you know kind of disasters in business history um and then um and then you know what became an internet depression on the
1:23:00
other side of that but then in that depression in the 2000s was the beginning of broadband and smartphones and Web 2.0 right and then social media
1:23:07
and search and every SAS and everything that came out of that so what did you learn from just the acquisition I mean
1:23:13
this is so much money what's interesting because I I must have
1:23:18
been very new to you that these software stuff you can make so much money there's
1:23:25
so much money swimming around I mean I'm sure the ideas of investment were starting to get born there yes let me
1:23:30
get so let me lay it so here's here's the thing I don't know if I figured out then but figured out later which is um software is a technology that it's
1:23:37
like a you know the concept of the Philosopher's Stone this is a philosopher stone in alchemy transmission led into gold and Newton
1:23:42
spent 20 years trying to find the Philosopher's Stone never got there nobody's ever figured it out software is our modern philosopher stone and in
1:23:49
economic uh terms it transmutes labor into capital which is like a super interesting thing
1:23:55
and by the way like Karl Marx is rolling over and describe right now because of course that's a complete refutation of his entire Theory um
1:24:02
transmission labor and capital which is which is as follows is somebody sits down on a keyboard and types A bunch of
1:24:08
stuff in and a capital asset comes out the other side and then somebody buys that Capital asset for a billion dollars
1:24:14
like that's amazing right it's literally creating value right out of thin air
1:24:19
right out of purely human thought right um and so that that that's there are
1:24:24
many things that make software magical and special but that's the economics I wonder what Marx would have thought about that oh he would have completely
1:24:30
broke his brain because of course the whole the whole thing was he was you could he you know that kind of
1:24:35
technology is inconceivable when he was alive it was all it was all industrial Hera stuff and so the any kind of Machinery necessarily involves huge
1:24:42
amounts of capital and then labor was on the on the receiving end of the abuse yeah um right but like software software a
1:24:48
software engineer is somebody who basically transmutes his own labor into action a national capital asset um creates permanent value well in fact
1:24:55
it's actually very inspiring um that's actually more true today than before so when I was doing software the Assumption was all new software basically has a
1:25:02
sort of a parabolic sort of life cycle right so you you ship the thing people buy it at some point everybody who wants
1:25:08
it has bought it and then it becomes Obsolete and it's like bananas nobody nobody buys old software
1:25:13
um these days um Minecraft um Mathematica you know Facebook Google
1:25:21
um you have the software assets that are you know have been around for 30 years that are gaining in value every year
1:25:26
right and they're just there being a World of Warcraft right salesforce.com like they're being every single year they're being polished and Polished and
1:25:32
Polished and Polished they're getting better and better more powerful more powerful more valuable more valuable so we've entered this era where you can
1:25:37
actually have these things that actually build out over decades which by the way is what's happening right now with like GPT
1:25:43
um and so um now and this is why you know there there is always you know sort of a constant investment frenzy around
1:25:49
software is because you know look when you start one of these things it doesn't always succeed but when it does now you
1:25:54
might be building an asset that builds value for you know four or five six decades to come um you know if you have a team of people
1:26:00
who have the level of devotion required to keep making it better and then the fact that of course everybody's online you know there's five
1:26:06
billion people that are a click away from any new piece of software so the potential Market size for any of these things is you know nearly infinite it
1:26:12
must have been surreal back then though yeah yeah this was all brand new right yeah back then this was all brand new these were all you know brand new had
1:26:19
you rolled out that theory in in even 1999 people would have thought you were smoking crack so that's that's emerged
1:26:25
over time well let's uh now turn back into the
Why AI will save the world
1:26:30
future you wrote the s-a-y-a-i will save the world let's start the very high level what's
1:26:36
the main thesis of the essay yeah so the main thesis on the essay is that what we're dealing with here is intelligence
1:26:42
um and it's really important to kind of talk about the sort of very nature of what intelligence is and
1:26:47
fortunately we have a we have a predecessor to machine intelligence which is human intelligence and we've
1:26:52
got you know observations and theories over thousands of years for what what what intelligence is in the hands of humans and what intelligence is right I
1:26:59
mean what it literally is is the way to uh you know capture process analyze synthesize information solve problems
1:27:05
um but the observation of of intelligence in human hands is that intelligence quite literally makes
1:27:11
everything better um and what I mean by that is every kind of outcome of like human quality of life
1:27:17
whether it's education outcomes or success of your children for a Career Success or health or
1:27:25
lifetime satisfaction um by the way propensity to peacefulness as opposed to
1:27:30
violence uh propensity for open-mindedness versus bigotry those are all associated with higher levels of
1:27:36
intelligence smarter people have better outcomes than almost as you write you know every domain of activity academic
1:27:42
achievement job performance occupational status income creativity physical health longevity learning new skills managing
1:27:49
complex tasks leadership entrepreneurial success conflict resolution reading
1:27:54
comprehension financial decision making understanding how this perspectives creative arts parenting outcomes and
1:28:00
life satisfaction one of the more depressing conversations I've had and I don't know why it's depressing I
1:28:07
have to really think through wise depressing but on IQ and uh the G Factor
1:28:15
and that that's something in large part is genetic
1:28:21
and it correlates so much with all of these things and success in life
1:28:27
it's like all the inspirational stuff we read about like if you work hard and so
1:28:32
on damn it sucks that you're born with the hand that you can't change but what if
1:28:38
you could you're saying basically a really important point I think it's a uh
1:28:43
in your articles it really helped me um
1:28:49
it's a nice added perspective to think about listen human intelligence the
1:28:54
science of intelligence is shown scientifically that it just makes life easier and better
1:29:00
the smarter you are and now let's look at artificial intelligence
1:29:05
and if uh that's a way to increase the the
1:29:12
some human intelligence then it's only going to make a better life yeah that's the argument and certainly at the
1:29:18
collective level we could talk about the collective effect of just having more intelligence in the world which which will have very big payoff but there's
1:29:23
also just at the individual level like what if every person has a machine you know and it's a concept of augment Doug
1:29:29
engelbert's concept of augmentation um you know what if everybody has a an assistant and the assistant is you know
1:29:36
140 IQ um and you happen to be 110 IQ
1:29:41
um and you've got you know something that basically is infinitely patient and knows everything about you and is
1:29:46
pulling for you in every possible way wants you to be successful and anytime you find anything confusing or want to
1:29:52
learn anything or have trouble understanding something or want to figure out what to do in a situation right I'm going to figure out how to prepare for a job interview like any of
1:29:59
these things like it will help you do it and it will therefore the combination will effectively be you know well if you
1:30:05
effectively raise your raise because it will effectively raise your IQ will therefore raise the odds of successful life outcomes in all these areas so
1:30:11
people below the this hypothetical 140 IQ it'll pull them up towards 140 IQ
1:30:17
yeah yeah and then of course you know people at people at 140 IQ will be able to have a peer right to be able to
1:30:22
communicate which is great and then people above 140 IQ will have an assistance if they can Farm things out to and then look God willing you know at
1:30:29
some point these things go from future versions go from 140 IQ equivalent to 150 to 160 to 180 right like Einstein
1:30:36
was the estimated to be on the order of 160. um you know so when we get you know 160
1:30:41
AI like will be you know when one assumes creating Einstein level breakthroughs and physics and and then
1:30:47
and then at 180 we'll be you know curing cancer and developing warp drive and doing all kinds of stuff and so it is
1:30:54
quite possibly the case this is the most important thing that's ever happened the best things ever happen because precisely because it's a lever on this
1:31:01
single fundamental factor of intelligence which is the thing that drives so much of everything else can you still man the case that human
1:31:08
plus AI is not always better than human for the individual you may have noticed that there's a lot of smart
1:31:13
running around sure yes right and so like it's smart there are certain people where they get smarter you know they get
1:31:19
to be more arrogant right so you know there's one huge flaw although to push back on that it might be interesting
1:31:25
because when the intelligence is not all coming from you but from a system from another system that might actually
1:31:31
increase the the amount of humility even in the one would hope
1:31:36
um or it could make more you know that's I mean that's that's for psychology to study yeah exactly another one is um the smart
1:31:44
people are very convinced that they you know have a more rational view of the world and that they have a easier time seeing through conspiracy theories and
1:31:50
hoaxes and right you know sort of crazy beliefs and all that there's a theory in Psychology which is actually smart
1:31:55
people so for sure people who aren't as smart are very susceptible to hoaxes and conspiracy theories but it may also be
1:32:00
the case that the smarter you get you become susceptible in a different way uh which is you become very good at marshalling facts to fit preconceptions
1:32:08
right um you become very very good at assembling whatever theories and Frameworks and pieces of data and graphs
1:32:15
and charts you need to validate whatever crazy idea has got in your head and so you're susceptible in a different way
1:32:21
right uh we're all sheep but different colored some sheep are better
1:32:26
at justifying it right um and those are the you know those are the smart shape right um so yeah look like it I would say this
1:32:32
look like there are no fantasy I'm not I'm not a utopian there are no panaceas in life um there are no like you know I
1:32:38
don't believe they're like pure positives I'm not a transcendental kind of person like that but you know so yeah there are going to be issues
1:32:44
um uh and um and you know look smart people another maybe you could say about smart people is they are more likely to get themselves in situations that are
1:32:50
you know beyond their grasp you know because they're just more confident that their ability to deal with complexity and their their eyes become bigger their
1:32:56
cognitive eyes become bigger than their stomach you know so yeah you could argue those eight different ways nevertheless
1:33:01
on that right clearly overwhelmingly again if you just extrapolate from what we know about human intelligence you're
1:33:07
you're improving so many aspects of life if you're upgrading intelligence so there'll be assistance at all stages
1:33:14
of life so when you're younger there's for Education all that kind of stuff or mentorship all of this and uh later on
1:33:22
as you're doing work and you've developed a skill and you're having a profession you'll have an assistant that helps you excel at that profession so at
1:33:29
all stages of Life yeah I mean look the theory is augmentations this is the gecko Birds term for the technical part made the subservation many many decades
1:33:35
ago that you know basically it's like you can have this oppositional frame of Technology where it's like us versus the machines but what you really do is you
1:33:41
use technology to augment human capabilities yeah and by the way that's how actually the economy develops that's we can talk about the economic side of
1:33:47
this but that's actually how the economy grows is through technology augmenting human human potential
1:33:53
um and so yeah and then you basically have a proxy or a you know or or a a um
1:33:58
you know a sort of prosthetic um you know so like you've got glasses you've got a wristwatch you know you've got shoes you know you've got these things
1:34:05
you've got a personal computer you've got a word processor you've got Mathematica you've got Google this is
1:34:11
the latest viewed through that lens the AI is the latest in a long series of basically augmentation methods uh to be
1:34:17
able to raise human capabilities it's just this one is the most powerful one of all because this is the one that goes directly to but what they call fluid
1:34:23
intelligence which is IQ well there's uh two categories of folks
Dangers of AI
1:34:30
that you outline that uh that worry about or highlight the risks of AI and you highlight a bunch of different risks
1:34:36
I would love to go through those risks and just discuss some brainstorm which ones are serious and which ones
1:34:43
are less serious but first the Baptist and the Bootleggers what are these two interesting groups of folks
1:34:49
who are who who worry about uh the effect of AI and human civilization or
1:34:56
say they do say okay yes I'll say they do the Baptist Warrior the Bootleggers
1:35:01
say they do yeah um so the Baptist and the Bootleggers is a metaphor from economics um from what's
1:35:07
called development economics and it's this observation that when you get social reform movements um in a society you tend to get two sets
1:35:14
of people showing up arguing for the social reform um and the the term baptism Bootleggers comes from the
1:35:19
American experience with alcohol prohibition um and so in the 1900s 1910s there was
1:35:25
this movement that was very passionate at the time which basically said alcohol is the evil and it's destroying Society
1:35:31
by the way there was a lot of evidence to support this um there were very high rates of very
1:35:36
high correlations then by the way and now between race of physical violence and alcohol use almost all violent
1:35:43
crimes have either the perpetrator or the victim are both drunk almost all if you see this actually in the work almost
1:35:48
all social harassment cases the workplace it's like at a company party and somebody's drunk like it's it's amazing how often alcohol actually
1:35:54
correlates to actually just dysfunction at least two domestic abuse and so forth child abuse and so you had this group of
1:36:00
people who were like okay this this is bad stuff and we should Outlaw it and those were quite literally Baptists those were super committed you know
1:36:06
hardcore Christian activists in a lot of cases there was this woman uh whose name was Carrie Nation
1:36:11
um who was this older woman who had been in this you know I don't know disastrous marriage or something and her husband had been abusive and drunk all the time
1:36:17
and she became the icon of the Baptist uh prohibitionist and she was legendary
1:36:22
in that era for carrying an ax um and doing you know completely on her own doing raids of saloons and like
1:36:28
picking her ax to all the bottles and things in the back and and so so a True Believer an absolute True Believer
1:36:35
um and with absolutely the purest of intentions and again there's a very important thing here it which is there's you could look at this
1:36:41
cynically and you could say the Baptists are like delusional you know extremists but you could also say look they're right like she was maybe you know she
1:36:47
had a point like she wasn't wrong um about a lot of what she said yeah but it turns out the way The Story Goes is
1:36:53
it turns out that there were another set of people who very badly wanted to outlaw alcohol in those days and those were the Bootleggers which was organized
1:36:59
crime this stood to make a huge amount of money if legal alcohol sales were banned and this was in fact the way the
1:37:06
history goes is this was actually the beginning of organized crime in the U.S this was the big Economic Opportunity that opened that up
1:37:12
um and so they went in together um and then they didn't go in together like the Baptist did not even necessarily know about the Bootleggers
1:37:18
because they were on their moral Crusade The Bootlegger certainly knew about the Baptists and they were like wow this is these people are like the great front
1:37:23
people for like you know it's good PR Shenanigans in the background yeah and they got the Volstead Act passed right
1:37:29
and they did in fact ban alcohol in the U.S and you'll notice what happened which is people kept drinking like it
1:37:35
didn't work and people kept drinking um the Bootleggers made a tremendous amount of money um and then over time it
1:37:41
became clear that it made no sense to make it illegal and it was causing more problems and so then it was revoked and here we sit with legal alcohol 100 years
1:37:48
later with all the same problems um and you know the whole thing was this like giant misadventure uh the Baptist
1:37:54
got taken advantage of by the bootleggers and the Bootleggers got what they wanted and and that was that the same two categories of folks are now uh
1:38:01
sort of suggesting that uh the development of artificial intelligence should be regulated 100 yeah it's the
1:38:06
same pattern and the the economists the same pattern every time like this is what happened with nuclear power this is what happened which is another
1:38:11
interesting one but like yeah this is this happens dozens and dozens of times um throughout the last hundred years and
1:38:17
and this is what's happening now and you write that it isn't sufficient to Simply
1:38:22
identify the actors and impugn their motives we should consider the arguments of both the Baptists and the Bootleggers
1:38:27
on their merits so let's do just that risk number one
1:38:34
uh will AI kill us all yes so
1:38:41
uh what do you think about this one this this what do you think is the core argument here
1:38:47
that uh the development of AGI perhaps but I said uh will destroy human
1:38:53
civilization no first of all you just did a sleight of hand because we went from talking about AI to AGI
1:38:59
is there a fundamental difference there I don't know what's AGI uh what's AI
1:39:04
what's inside I know what AI is AI is machine learning I think we don't know what the bottom of
1:39:10
the well of machine learning is or what the ceiling is right because just uh to call something machine learning or just
1:39:15
to call some of the statistics or just to call it math or computation doesn't mean you know uh nuclear weapons are
1:39:21
just physics so it's it it's to me it's very interesting and surprising how far
1:39:27
machine learning no but we knew that nuclear physics would lead to weapons that's why the scientists of that era were always in this huge dispute about
1:39:33
building the weapons this is different ages different as machine learningly do we know we don't know but this is my point it's different we actually don't
1:39:39
know but and this is where you the sleight of hand kicks in right this is where it goes from being a scientific topic to being a religious topic
1:39:46
um and that's that's why I specifically called out that because that's what happens they do the vocabulary shift and all of a sudden you're talking about something totally that's not actually
1:39:52
real well then maybe you could also uh as part of that Define the Western tradition of millennialism yes end of
1:40:00
the world apocalypse apocalypse Cults um well so we live in we of course live in
1:40:07
a judeo Christian but primarily Christian kind of saturated you know kind of Christian post-christian secularized Christian you know kind of
1:40:12
world in the west um and of course Court of Christianity is the idea of the second coming and and
1:40:17
you know the revelations and you know Jesus returning and thought the Thousand-Year you know Utopia on Earth and then the you know the Rapture and
1:40:23
like all stuff you know we don't we you know we collectively you know as a society we don't necessarily take all
1:40:28
that fully seriously now so what we do is we create our secularized versions of that we keep we keep looking for Utopia
1:40:34
we keep looking for you know basically the end of the world and so what you see over over decades is basically a pattern
1:40:39
of these sort of of these of these of is this this is what calls are this is how Cults form as they form around some
1:40:45
theory of the end of the world and so the people's Temple calls the Manson called the heavens gate called the David
1:40:51
koresh called it you know what they're all organized around is like there's going to be this thing that's going to happen that's going to basically bring
1:40:56
civilization crashing down and then we have this special Elite group of people who are going to see it coming and prepare for it and then they're the
1:41:03
people who are either going to stop it or are failing stopping it they're going to be the people who survive to the other side and ultimately get credit for
1:41:08
having been right why is that so compelling do you think like uh because it satisfies this very deep need we have
1:41:14
for Transcendence and meaning that got Stripped Away when we became
1:41:19
secular yeah but why is the Transcendence involved the destruction of human civilization because like how
1:41:26
like how plausible it's it's like a very deep psychological thing because it's like how plausible how plausible is it
1:41:32
that we live in a world where everything's just kind of all right how exciting is that right but that's
1:41:38
more than that but that's the Deep question I'm asking why is it not exciting to live in a world where
1:41:44
everything's just all right does it I think uh you know most of the animal kingdom would be so happy we just all
1:41:51
right yeah because that means survival why are we uh maybe that's what it is why are we Conjuring up things to worry
1:41:59
about so C.S Lewis called it the god-shaped hole so there's a god-shaped
1:42:04
hole in The Human Experience Consciousness Soul whatever you want to call it where there's got to be
1:42:09
something that's bigger than all this there's got to be something Transcendent there's got to be something that is bigger right bigger the bigger purpose a
1:42:16
bigger meaning and so we have run the experiment of you know we're just going to use science and
1:42:21
rationality and kind of you know everything's just going to kind of be as it appears and a large number of people
1:42:26
have found that very deeply wanting and have constructed narratives and by the way this is the story of the 20th
1:42:32
century right communism right was one of those communism was a was a form of this Nazism was a form of this
1:42:38
um you know some people um you know you can see movements like this playing out all over the world right now so you
1:42:43
could start the kind of devil a kind of source of evil and we're going to transcend Beyond it yeah and the
1:42:49
millinarians the Millionaire's kind of when you see a millionaire and cult they put a really specific point on it which
1:42:55
is end of the world right there there is some change coming and that change that's coming is so profound and so
1:43:01
important that it's either going to lead to Utopia or hell on Earth right um and it is going to and then you know
1:43:08
it's like what if you actually knew that that was going to happen right what would you what what would you do right
1:43:14
how would you prepare yourself for it how would you come together with a group of like-minded people right how would you what would you do would you plan
1:43:20
like caches of weapons in the woods would you like you know I don't know if it's create under underground bunkers would you you know spend your life
1:43:26
trying to figure out a way to avoid having it happen yeah that's a really compelling exciting idea to uh to have a
1:43:33
club over to have to have a to have a little bit of trial like a get together on a Saturday night and drink some beers
1:43:39
and talk about the the end of the world and how are you you are the only ones who have figured it out yeah and then
1:43:44
and then once you lock in on that like how can you do anything else with your life like this is obviously the thing that you have to do and then and then there's a psychological effect you
1:43:51
alluded to there's a psychological effect if you take a set of True Believers and you leave them to themselves they get more radical right because they self-radicalize each other
1:43:57
that said yes it doesn't mean they're not sometimes right yeah at the end of
1:44:02
the world might be yes correct like they might be right yeah but like we have some pamphlets for you
1:44:09
I mean there's I mean we'll talk about nuclear weapons because if you have a really interesting little moment that I
1:44:14
learned about in in your essay but you know sometimes it could be right yeah yeah because we're still you were
1:44:20
developing more and more powerful Technologies uh in this case and we don't know what the impact they will
1:44:25
have on human civilization well we can highlight all the different predictions about how it will be positive but the
1:44:30
risks are there and you discuss some of them well the Steel Man the steel man is the Steel Man
1:44:36
actually the steel man and his refutation are the same which is you can't predict what's going to happen right you right you can't rule out that
1:44:42
this will not end everything right but the response to that is you have just made a completely non-scientific claim
1:44:47
yeah you've made a religious claim not a scientific claim how does it get disproven there is and there's no by definition with these kinds of claims
1:44:53
there's no way to disprove them yeah right um and so there's no you just go right on the list there's no hypothesis
1:44:58
there's no testability of the hypothesis there's no um way to falsify the hypothesis there's no way to measure
1:45:05
progress along the arc like it's just all completely missing and so it's not
1:45:10
scientific and well I don't think it's completely missing it's it's somewhat missing so for example the the the the
1:45:16
people that say AI is going to kill all of us I mean they usually have ideas about how
1:45:21
to do that whether it's the paperclip maximizer or um you know it escapes
1:45:27
there's mechanism by which you can imagine it killing all humans models and
1:45:33
to you can disprove it by saying there is um there's a limit
1:45:39
to uh the speed at which intelligence increases maybe show that uh
1:45:46
like the sort of rigorously really described model like how it could happen
1:45:52
and say no there here's a physics limitation there's a physical limitation
1:45:58
to how these systems would actually damaging human civilization and it is possible they will kill 10 to 20 percent
1:46:04
of the population but it seems impossible for them to kill uh 99 there's practical counter arguments
1:46:10
right so you mentioned basically what I described as the thermodynamic counter argument which is sitting here today it's like we're with the evil AGI gets
1:46:16
the gpus yeah because like they don't exist so you're gonna have a very frustrated baby evil AGI who's going to
1:46:21
be like trying to buy Nvidia stock or something to get them to finally make some chips um right so the the serious
1:46:26
form of that is the thermodynamic argument which is like okay where's the energy going to come from where's the processor going to be running where is
1:46:31
the data center going to be happening how is this going to be happening in Secrets such that you know it's not you know so so that's a practical counter
1:46:37
argument to The Runaway AGI I think I have a but I have a and we can argue that discuss that I have a deeper objection to it which is it's this is
1:46:43
all forecasting it's all modeling it's all it's all future prediction it's all future hypothesizing it's not science
1:46:51
sure it is not it is it is the opposite of science so the Carl Sagan extraordinary claims require
1:46:57
extraordinary proof right these are extraordinary claims the policies that are being called for right to prevent
1:47:03
this are of extraordinary magnitude and I think we're going to cause extraordinary damage and this is all
1:47:09
being done on the basis of something that is literally not scientific it's not a testable hypothesis so the moment you say AI is going to kill all of us
1:47:15
therefore we should ban it or that we should regulate all that kind of that's when it starts getting serious or start
1:47:20
you know military airstrikes and data centers oh boy right and like
1:47:26
yeah this one is get starts well so it starts getting real so here's the problem of millionaire and Cults they
1:47:31
have a hard time staying away from violence HIPAA violence is so fun
1:47:38
if you're on the right end of it they have a hard time avoiding balance the reason I have a hard time affording balance is if you actually believe the
1:47:44
claim right then what would you do to stop the end of the world well you would do anything right yeah and so and this
1:47:52
is where you get I mean again if you just look at the history of millionaire and Cults this is where you get the people's Temple and everybody killing
1:47:57
themselves in the jungle and this is where you get Charles Manson and you know sending in need to kill kill the pigs like this is the problem with these
1:48:03
they have a very hard time to run the line at actual violence and I think I think in this case there's there I mean
1:48:09
they're already calling for it like today and you know where this goes from here is they get more worked up like I
1:48:14
think it's like really concerning okay but that's kind of the extreme so you know the extremes of anything I was
1:48:20
concerning it's also possible to kind of believe that AI has a very high likelihood of
1:48:26
killing all of us uh but there's and therefore we should uh maybe consider
1:48:32
uh slowing development or regulating so not violence or any of these kinds of things but it's saying like all right
1:48:39
let's let's take a pause here you know your biological weapons nuclear weapons like
1:48:44
this is like serious stuff we should be careful so it is possible to kind of
1:48:50
have a more rational response right if you believe this risk is real believe yes so what is it possible to be have a
1:48:58
scientific approach to the the prediction of the future I mean we just went through this with covet yeah what
1:49:04
do we know about modeling well I mean what do we learn about modeling with covet uh there's a lot of
1:49:10
lessons they didn't work at all they worked poorly the models were terrible the models were useless I don't
1:49:16
know if the models were useless or the people interpreting the models and then
1:49:21
the centralized institutions that were creating policy rapidly based on the models and leveraging the models in
1:49:27
order to support their narratives versus actually interpreting the error bars and the bottles and all that kind of stuff
1:49:33
what you had with my view you had with confidence you have these experts showing up and they claim to be scientists and they had no testable
1:49:40
hypotheses whatsoever they had a bunch of models they had a bunch of forecasts and they had a bunch of theories and they laid these out in front of policy
1:49:45
makers and policymakers freaked out and panicked right and implemented a whole bunch of like really like terrible decisions that were still living with
1:49:52
the consequences of um and there was never any empirical Foundation to any of the models none of
1:49:57
them ever came true yeah to push to push back they were certainly Baptists and Bootleggers in this in the context of
1:50:02
this pandemic but there's still a usefulness to models no I I so not if they're I mean not if they're reliably
1:50:08
wrong right then they're actually like anti-useful right they're actually damaging but what do you do with a pen endemic what do you do with the were
1:50:14
they at with any kind of threat don't you want to kind of have um several models to play with as part of the
1:50:21
discussion of like what the hell do we do here I mean do they work because they're an expectation that they
1:50:27
actually like work that they have actual predictive value I mean as far as I can tell with covet we just saw the policy makers just Psy
1:50:33
up themselves into believing that there was I mean look the scientists the scientists were at fault this is the quote unquote scientists showed up so I
1:50:39
had some insight into this so there was a remember the Imperial College models out of London were the ones that were like these are the gold standard models
1:50:45
yeah so a friend of mine runs a big software company and he was like wow this is like hope it's really scary and he's like you know he contacted This
1:50:51
research and he's like you know do you need some help you've been just building this model on your own for 20 years do you need some who did you like our
1:50:56
coders to basically restructure it so it can be fully adapted for covet and the guy said yes and sent over the code and
1:51:02
my friend said it was like the worst spaghetti code he's ever seen that doesn't mean it's not possible to construct a good model of pandemic with
1:51:08
the correct error bars with a high number of parameters that are continuously many times a day updated as
1:51:14
we give more data about a pandemic I would like to believe when a pandemic hits the world the best computer
1:51:21
scientist in the world the best software Engineers respond aggressively and as
1:51:26
input take the data that we know about the virus and put say here is here's
1:51:32
what's happening in terms of how quickly it's spreading what that lead in terms of hospitalization and deaths and all
1:51:38
that kind of stuff here's How likely how contagious it likely is here's how deadly likely is based on different
1:51:44
conditions based on different ages and demographics and all that kind of stuff so here's the best kinds of policy it
1:51:50
feels like you could have models machine learning that like kind of they
1:51:57
don't perfectly predict the future but they they help you do something because there's pandemics that are like
1:52:04
uh meh they don't really do much harm and there's pandemics you can imagine them
1:52:10
they could do a huge amount of harm like they can kill a lot of people so you
1:52:16
should probably have some kind of data driven models that keep updating that allow you to make decisions on base like
1:52:22
where how bad is this thing uh now you can criticize how horrible all that went with the
1:52:29
response to this pandemic but I just feel like there might be some value to models so to be useful at some point it has to be predictive right so and so and
1:52:36
and so the easy thing for me to do is to say obviously you're right obviously I want to see that just as much as you do
1:52:41
because anything that makes it easier to navigate through Society through a wrenching you know risk like that is you know that sounds great um you know the
1:52:47
the harder objection to it is just simply you are trying to model a complex dynamic system with eight billion moving
1:52:53
Parts like not possible very tough can't be done complex systems can't be done uh machine learning says hold my beer but
1:53:00
well it's possible no I don't know I would like to believe that it is yeah put it this way I think where you and I
1:53:05
would agree is I think we would like we would like that to be the case we are strongly in favor of it I think we would
1:53:11
also agree that no such thing with respect to covet or pandemics no such thing at least neither you nor I think or where I'm not aware of anything like
1:53:17
that today my main worry with the response to the pandemic is that uh uh
1:53:23
same as with aliens is that even if such a thing existed and it's possible it
1:53:28
existed the the the the policy makers we're not paying attention like uh there was no
1:53:35
mechanism that allowed those kinds of models to percolate up oh I think we have the opposite problem during covet I think the policymakers I think the these
1:53:42
these these people with basically fake science had too much access to the policymakers well right and what but the policy
1:53:48
makers also wanted they had a narrative in mind and they also wanted to use whatever model that fit that narrative
1:53:54
to help them out so like it felt like there was a lot of politics and not enough science yeah although a big part of what was happening a big reason we
1:54:00
got lockdowns for as long as we did was because these scientists came in with these like Doomsday scenarios that were like just like completely off the hook
1:54:05
scientists and quotes that's not quote-unquote that's not okay let's give love so here's science that is the way
1:54:11
out science is a process of testing hypotheses yeah modeling does not involve testable hypotheses right like I
1:54:18
don't even know that my I actually don't I didn't even know that modeling actually qualifies in science maybe that's a side conversation we
1:54:24
could have some time over a beer uh it's a really interesting but what do we do about the future I mean what what so
1:54:29
number one is when we start with number one humility it goes back to this thing of how do we determine the truth number
1:54:34
two is we don't believe you know it's the old I've got a hammer everything looks like a nail right um I've got oh uh it's one of the
1:54:40
reasons I gave you I gave Alex a book um uh which is the topic of the book is what happens when scientists basically stray off the path of technical
1:54:47
knowledge and start to weigh in on politics and societal issues in this case philosophers philosophers but he
1:54:53
actually talks in this book about like I talked about the nuclear age and Einstein he talks about the physicists uh actually uh doing doing uh very
1:55:00
similar things at the time the book is when reason goes on holiday philosophers and politics by uh Nevin and it's just a
1:55:07
story it's a story there's there are other books on this topic but this is a new one that's really good it's just a story of what happens when experts in a
1:55:13
certain domain decide to weigh in and become basically social engineers and uh and the political um you know basically
1:55:18
political advisors and it's just a story of just unending catastrophe right and I think that's what happened with covet again
1:55:24
yeah I found this book a highly entertaining and eye-opening read filled with the amazing anecdotes of irrationality and craziness by famous
1:55:31
recent philosophers after you read this book you will not look at Einstein the same oh boy yeah don't destroy my heroes
1:55:38
anymore um I'm sorry you shouldn't read the book all right but here's the thing the AI
1:55:45
the AI risk people they don't even have the covet model at least not that I'm aware of no like
1:55:51
there's not even the equivalent of the government model they don't even have the spaghetti code they've got a theory and a warning and a
1:55:57
this and that and like if you ask like okay well here's here's the I mean the ultimate example is okay how do we know right how do we know that an AI is
1:56:04
running away like how do we know that the film takeoff thing is actually happening and the only answer that any of these guys have given that I've ever
1:56:09
seen is oh it's when the loss rate the loss uh function and the training drops
1:56:15
right that's when you need to like shut down the data center right and it's like well that's also what happens when you're successfully training a model
1:56:20
like like what what even is this is not science this is not it's not anything
1:56:27
it's not a model it's not anything there's nothing to arguing with it it's like you know pushing Jello like there's what do you even respond to uh so did
1:56:34
you put pushback on that I don't think they have good metrics of yeah wonderful is
1:56:39
happening but I think it's possible to have that like I just just as you speak now I mean it's possible to imagine
1:56:45
there could be measures it's been 20 years no for sure but it's been only weeks since we had a big enough
1:56:52
breakthrough in language models we can start to actually have this the thing is the AI Doomer stuff didn't have any
1:57:00
actual systems to really work with and now there's real systems you can start to analyze like how does this stuff go
1:57:05
wrong and I think you kind of agree that there is a lot of risks that we can analyze the benefits outweigh the risks
1:57:10
in many cases but the risks are not existential yes well not not in the food
1:57:15
not in the phone paper clip now so let me okay there's another sleight of hand that you just alluded to there's another sleight of hand that happens which is very I think I'm very good at the
1:57:21
sleight of hand thing which is very not scientific so the book super intelligence right which is like the
1:57:27
Nick bostrom's book which is like the origin of a lot of this stuff which was written you know whatever 10 years ago or something so he does this really
1:57:32
fascinating thing in the book which is he basically says um uh there are many possible routes to
1:57:38
machine intelligence to artificial intelligence and he describes all the different routes to artificial intelligence all the different possible
1:57:44
everything from biological augmentation through to you know done all these different things um one of the ones that he does not
1:57:50
describe is large language models because of course the book was written before they were invented and so they didn't exist
1:57:56
in the book he just he describes them all and then he proceeds to treat them all as if they're exactly the same thing he presents them all as sort of an
1:58:03
equivalent risk to be dealt with in an equivalent way to be thought about the same way and then the risk the quote-unquote risk that's actually emerged is actually a completely
1:58:09
different technology than he was even imagining and yet all of his theories and beliefs are being transplanted by this movement like straight on this new
1:58:15
technology and so again like there's no other area of science or technology where you do that yeah like when you're
1:58:21
dealing with like organic chemistry versus inorganic chemistry you don't just like say oh with respect to like
1:58:27
either one basically maybe you know growing up and eating the world or something like they're just going to operate the same way like you don't but
1:58:32
you can start talking about like as as we get more and more actual systems that start to get more and more intelligent
1:58:38
you can start to actually have more scientific arguments here like you know high level you can talk about the threat
1:58:45
of autonomous weapon systems back before we had any Automation in in the military and that would be like very fuzzy kind
1:58:51
of logic but the more and more you have drones that are becoming more and more autonomous you can start imagining okay
1:58:58
what does that actually look like and what's the actual threat of autonomous weapon systems How does it go wrong and
1:59:03
still it's it's very vague but you start to get a sense of like all right
1:59:09
um it should probably be illegal or wrong or not allowed to do like
1:59:15
Mass deployment of fully autonomous drones that are doing aerial strikes oh
1:59:21
no on large areas I think it should be required right so that's no no I think if you're required that only
1:59:27
aerial vehicles are automated okay so you want to go the other way I want to go the other way so that okay I
1:59:34
think it's obvious that the machine is going to make a better decision than the human pilot I think it's obvious that it's in the
1:59:40
best interest of both the attacker and the defender and Humanity at large if machines are making more of these decisions and not people I think people make terrible decisions in times of War
1:59:47
but like there's a there's ways this can go wrong too right well the words go terribly wrong now
1:59:53
this goes back to the this is that whole thing about like this Ultra it is the self-driving car need to be perfect versus does it need to be better than
1:59:58
the human driver yeah does the automated drone need to be perfect or does it need to be better than a human pilot at
2:00:04
making decisions under enormous amounts of stress and uncertainty yeah well the on average
2:00:09
the the worry that AI folks have is the runaway they're going to come alive right then
2:00:15
again that's the sleight of hand right or not not come alive well no hold on a second you're becoming do you lose
2:00:21
control but then they're going to develop goals of their own they're going to develop them under their own they're
2:00:26
going to develop their own right no more more like uh Chernobyl style meltdown like uh just bugs in the code
2:00:34
accidentally you know Force you the the results in the bombing of like large
2:00:41
civilian areas okay and to a degree that's not possible um in in the current uh military
2:00:48
strategies well actually we've been doing a lot of mass bombings to cities for a very long time yes and a lot of
2:00:54
civilians died a lot of civilians died and if you watch the documentary The Fog of War McNamara it's been a big part of
2:01:00
it talking about the firebombing of the Japanese cities yeah bringing them straight to the ground right the devastation in Japan American Military
2:01:06
uh firebombing the cities in Japan was considerably bigger Devastation than the use of nukes right so we've been doing
2:01:12
that for a long time we also did that Germany by the way Germany did that to us right like that's an old tradition
2:01:17
the minute we got airplanes we started doing indiscriminate bombing so one of the things we're still doing it the modern U.S uh military can do with
2:01:25
technology with automation but technology more broadly is uh higher and higher Precision strikes yeah well so
2:01:31
Precision is obviously and this is the the jdam right so there's this big Advance this big Advance um called the
2:01:36
JDM which basically was strapping a GPS transceiver to uh to a to an unguided bomb and turning it into a guided guided
2:01:42
bomb and yeah that's great like look that's been a big advance but and that's like a baby version of this question which is okay do you want like the human
2:01:48
pilot like guessing where the bomb's gonna land or do you want like the machine like getting the bottom twist destination that's a baby version of the question
2:01:54
the next version of the question is do you want the human or the machine deciding whether to drop the bomb everybody just assumes the human's going
2:01:59
to do a better job for what I think are fundamentally suspicious reasons emotional psychological reasons I think
2:02:04
it's very clear that the machine's going to do a better job making that decision because the humans making that making that decision are god-awful just
2:02:10
terrible yeah right and so so yeah so this is the this is the thing and then let's get to the there's can I one more
2:02:15
slide of hand okay please I'm a magician you could say one more sleight of hand these things are going to be so smart
2:02:22
right that they're going to be able to destroy the world and wreak havoc and like do all this stuff and plan and do all the stuff and evade us and have all
2:02:28
their secret things and their secret factories and all this stuff but they're so stupid that they're going to get like
2:02:33
tangled up in their code and that's they're not going to come alive but there's going to be some bug that's going to cause them to like turn us all into paper like that they're not gonna
2:02:39
they're gonna be genius in every way other than the actual bad goal and it's just like and that's just like
2:02:45
a like ridiculous like discrepancy and and and you can prove this today you can
2:02:50
actually address this today for the first time with llms which is you can actually ask llms to resolve uh moral
2:02:56
dilemmas yeah so you can create the scenario you know dot dot dot this that this that this that what would you as
2:03:02
the AI do in the circumstance and they don't just say Destroy All Humans Destroy All Humans they will give you actually very nuanced moral practical
2:03:10
trade-off oriented answers so we actually already have the kind of AI that can actually like think this
2:03:16
through and can actually like you know reason about goals well the the hope is that AGI or like a very super
2:03:23
intelligent systems have some of the Nuance that llms have and the intuition is they most likely will because even
2:03:28
these llms have the Nuance uh elements are really this is actually worth worth
2:03:34
um spending moment on LMS are really interesting to have moral conversations with and that
2:03:39
is I didn't expect I'd be having a moral conversation with a machine in my lifetime well and let's remember we're
2:03:45
not really having a conversation with a machine where we're having a conversation with the entirety of the collective intelligence of the human
2:03:50
species exactly yeah correct but it's possible to imagine autonomous weapon systems that are not using llms but if
2:03:58
they're smart enough to be scary why are they not smart enough to be wise
2:04:04
like that's the part where it's like I don't know how you get the one without the other is it possible to be super intelligent without being Super Wise
2:04:11
well you're again you're back to that I mean then you're back to a classic autistic computer right like you're back to just like a blind rule follower I've
2:04:18
got this like core is the paperclip thing I've got this core Rule and I'm just going to follow it to the end of the Earth and it's like well but everything you're going to be doing
2:04:23
execute that rule is going to be super genius level that humans aren't going to be able to counter it's just a it's a it's a mismatch in the definition of
2:04:29
what the system is capable of unlikely but not impossible I think but again here you get to like okay like no I'm
2:04:36
not saying when it's unlikely but not impossible if it's unlikely that means the the fear should be correctly
2:04:43
calibrated extraordinary claims require extraordinary proof well okay so uh one interesting sort of tangent I would love
Nuclear energy
2:04:49
to take on this because you mentioned this in the essay about nuclear which was also I mean you don't shy away from a little
2:04:55
bit of uh of a spicy take so uh uh Robert Oppenheimer famously
2:05:01
said no I am become death the destroyer of worlds as he witnessed the first detonation of a nuclear weapon on July
2:05:08
16th 1945 and you write an interesting historical perspective uh quote recall
2:05:14
that John Von Neumann responded to Robin Robert oppenheimer's famous handwringing about the role of creating nuclear
2:05:20
weapons which you note helped end World War II and prevent World War III
2:05:27
with some people confess guilt to claim credit for the sin and you also
2:05:32
mentioned that Truman was harsher after meeting Oppenheimer he said that uh don't let that crybaby be in here again
2:05:39
real quick real quick by the way promoting it from Dean Atchison oh boy because Oppenheimer didn't just
2:05:47
say the famous line yeah he then spent years going around basically moaning him you know going on TV and going into
2:05:52
going into the White House and basically like just like doing this hair shirt you know thing self you know this sort of self-critical like oh my God I can't
2:05:58
believe how awful I am so he's the the he's widely considered perhaps because of the Hang ringing is
2:06:05
the father of the atomic bomb um this is my name is criticism of him
2:06:11
as he tried to have his cake and eat it too like he wanted to in in soyman of course a very different kind
2:06:16
of personality and he's just like yeah this is like an incredibly useful thing I'm glad we did it yeah well if I know
2:06:22
him as as widely um credit is being one of the smartest humans of the 20th century there's
2:06:28
certain certain people everybody says like this is the smartest person I've ever met when they've met him anyway uh
2:06:34
that doesn't mean smart doesn't mean wise so the guy I would love to sort of can
2:06:41
you make the case both for and against the critique of Oppenheimer here because we're talking about nuclear weapons boy
2:06:49
do they seem dangerous well so the critique goes deeper and I left this out here's the real substance I left it out
2:06:54
because I didn't want to dwell on nukes in my paper but here's the deeper thing that
2:07:00
happened and I'm really curious this movie coming out this summer I'm really curious to see how far he pushes this because this is the real drama in the
2:07:05
story which is it wasn't just a question of our nukes good or bad it was a question of should Russia also have them
2:07:10
um and what what actually happened um was Russia got the America invented the bomb Russia got the bomb they got the
2:07:17
bomb through Espionage they got American and you know they got American scientists and foreign scientists working on the American project some
2:07:23
combination of the two uh basically gave the Russians the designs for the bomb and that's how the Russians got the bomb
2:07:30
um there's this dispute to this day of oppenheimer's role in that um if you read all the histories the
2:07:36
kind of composite picture and by the way we now know a lot actually about Soviet Espionage in that era because there's been all this Declassified material in
2:07:42
the last 20 years that actually shows a lot of a lot of very interesting things but if you kind of read all the histories that you kind of get is
2:07:47
Oppenheimer himself probably was not a he probably did not hand over the nuclear secrets himself however he was
2:07:52
close to many people who did yeah including family members and there were other members of the Manhattan Project who were Russian Soviet assets and did
2:08:00
hand over the bomb and so the view of that Oppenheimer and people like him had
2:08:05
that this thing is awful and terrible and oh my God and you know all this stuff you could argue argue fed into
2:08:11
this ethos at the time that resulted in people thinking that the Baptists thinking that the only principal thing to do was to give the Russians the bomb
2:08:18
um and so the the moral beliefs on this thing and the public discussion and the role that the inventors of this
2:08:24
technology play this is the point of this book when they kind of take on this sort of public intellectual moral kind
2:08:29
of thing it can have real consequences right because we live in a very different world today because Russia got the bomb than we would have lived in had
2:08:35
they not gotten the bomb right the entire 20th century second half the 20th century would have played out very different had those people not given a
2:08:41
rush of the bomb and so the stakes were very high then the good news today is
2:08:47
nobody's sitting here today I don't think worrying about like an analogous situation with respect to like I'm not really worried that Sam Walton was going
2:08:53
to decide to give you know the Chinese the design for yeah although he did just speak at a Chinese conference which is interesting
2:08:59
but however I don't think I don't think that's what's at play here but what's at play here are all these other fundamental issues around what do we
2:09:05
believe about this and then what laws and regulations and restrictions that we're going to put on it and and that's where I draw like a direct straight line
2:09:11
and anyway and my reading of the history on nukes is like the people who were doing the full hair shirt public this is
2:09:17
awful this is terrible actually had like catastrophically bad results uh from from taking those views um and that's
2:09:23
what I'm worried it's gonna happen again but is there a case to be made that you really need to wake the public up to the dangers of nuclear weapons when they
2:09:29
were first dropped like really like educate them on like this is extremely dangerous and destructive weapon I think
2:09:36
the education kind of happened quick and early like wow it was pretty obvious wow we dropped one bomb and destroyed an
2:09:41
entire city yeah so 80 000 people then what uh hey
2:09:47
look but I don't like the reporting of that you can report that in all kinds of ways of course you can you can do all
2:09:53
kinds of Slants like war is horrible war is terrible you can do you can make it seem like nuclear the use of nuclear
2:10:00
weapons is just a part of war and all that kind of stuff something about the reporting in the discussion of nuclear
2:10:06
weapons resulted in us being terrified in awe of the power of nuclear
2:10:12
weapons and that potentially fed in a positive way towards the the
2:10:18
game theory of mutually shared destruction well so this gets to what actually happened let's get to what some of us Playing devil's advocate here yeah
2:10:25
yeah sure of course let's get to what actually happened and then kind of back into that so what actually happened I believe and again I think this is a
2:10:30
reasonable reading of history is what actually happened was nukes then prevented World War III and they prevented World War III through the game
2:10:36
theory of mutually assured destruction had nukes not existed right there would have been no reason why the cold war did
2:10:42
not go hot right and then they're and then you know in the military planners at the time right thought both on both sides thought that there was going to be
2:10:48
World War III on the planes of Europe and they thought there was going to be like 100 million people dead right it was like the most obvious thing in the world to happen
2:10:54
right and it's the dog that didn't bark right like it may be like the best single net thing that happened in the
2:10:59
entire 20th century is it like that didn't happen yeah actually just at that point you say a lot of really brilliant
2:11:04
things it hit me just as you were saying it I don't know why it hit me for the first
2:11:10
time but we got two Wars in a span of like uh 20 years
2:11:17
like we could have kept getting more and more world wars more and more ruthless it actually you could have had a US
2:11:24
versus Russia War you could have by the way you have there's another hypothetical scenario the other
2:11:29
hypothetical scenario is the Americans got the bomb the Russians didn't right and then America's the big dog and
2:11:34
then maybe America would have had the capability to actually roll back the Iron Curtain right now I don't know whether that
2:11:39
would have happened but like it's entirely possible right and and and the act of these people who have these moral
2:11:45
positions about because they could forecast they could model they could forecast the future of how this technology would get used made a horrific mistake because they basically
2:11:51
ensured that the Aaron Curtin would continue for 50 years longer than it would have otherwise and again like these are counterfactuals I don't know that that's what would have happened but
2:11:58
like the decision to hand the bomb over was a big decision
2:12:04
made by people who were very full of themselves yeah but so me as an America me as a
2:12:09
person that loves America I also wonder if us was the only ones with the nuclear weapons
2:12:16
uh that was the argument for Handy that was the was uh the guys who the guys who handed over the bomb that was actually
2:12:22
their moral argument yeah I would I would probably not hand it over to I would be careful about the regimes you
2:12:28
handed over to maybe you give it to like the British or something or like uh like a democratically elected
2:12:36
government well look there are people to this day who think that those Soviet spies did the right thing because they created a balance of Terror as opposed
2:12:42
to the us having just and by the way let me let me balance of Terror let's tell the full version that's such a sexy ring to it okay so the full version of the
2:12:48
story is John Von Neiman's a hero of both Heroes and mine the full version of the story is he advocated for a first strike so when the U.S had the bomb and
2:12:56
Russia did not he advocated for he said we need to strike them right now strike Russia yeah
2:13:03
yes because he said World War III is inevitable
2:13:09
um he was very hardcore uh he his his theory was um his theory was World War III is
2:13:14
inevitable we're definitely going to have world war three the only way to stop world war three is we have to take them out right now and we have to take them out right now before they get the
2:13:20
bomb because this is our last chance now again like is this an example of philosophers in politics I don't know if
2:13:26
that's in there or not but this is in the standard box no but it is it meaning yeah this is on the other side so so most of the case studies most of the
2:13:32
case studies in books like this are the crazy people on the left yeah um Von Neumann is a story arguably of
2:13:37
the crazy people on the right um yeah stick to Computing John well this is the thing and this is this is the general principle is it going to
2:13:44
affect our core thing which is like I don't know whether any of these people should be making any of these calls yeah because there's nothing in either Von
2:13:50
Neumann's background or oppenheimer's background or any of these people's background that qualifies them as moral authorities yeah well this actually
2:13:56
brings up the point of Nai who are the good people to to reason about the
2:14:01
morality the ethics the outside of these risks outside like the more complicated stuff that you you agree on is you know
2:14:08
this will go into the hands of bad guys and all the kinds of ways they'll do is is interesting and dangerous
2:14:15
um is dangerous and interesting unpredictable ways and who is the right person who are the right kinds of people
2:14:21
to make decisions how respond to it is attack people so the history of these
2:14:26
fields this is what he talks about in the book The History of these fields is that the the competence and capability
2:14:32
and intelligence and training and accomplishments of senior scientists and technologists working on a technology
2:14:38
and then being able to then make moral judgments in the use of the technology that track record is terrible
2:14:43
that tracker that track record is like catastrophically bad um and the people just illuminate the
2:14:48
people that developed that technology are usually not going to be the right people well why would they so the claim is of
2:14:55
course they're the knowledgeable ones but the problem is they've spent their entire life in a lab right they're not
2:15:00
theologians but so what you find what you find when you read when you read this when you look at these histories
2:15:05
which you find is they generally are very thinly informed on history on sociology on on on theology on morality
2:15:12
ethics they tend to manufacture their own world views from scratch they tend
2:15:17
to be very sort of thin um they're not remotely the arguments that
2:15:24
you would be having if you got like a group of Highly qualified theologians or philosophers or you know um well let me
2:15:29
uh sort of uh as The Devil's Advocate takes a sip of whiskey say that
2:15:34
I I agree with uh with that but also it seems like the
2:15:40
people who are doing kind of the ethics departments and these tax tech companies go sometimes the other way yes
2:15:48
they're not nuanced on the on history or theology or this kind of stuff they it
2:15:54
almost becomes a kind of outraged activism towards um
2:15:59
directions that don't seem to be grounded in history and humility and
2:16:04
Nuance it's again drenched with arrogance so so definitely I'm not sure which is worse oh no they're both bad
2:16:11
yeah so definitely not them either so but I guess what this is a hard yeah
2:16:16
it's a hard problem that's not a problem and this goes back to where we started which is okay who has the truth and it's like well
2:16:21
um you know like how do Society survive at like truth and how do we figure these things out and like our elected leaders play some role in it
2:16:28
you know we all play some role in it um there have to be some set of public intellectuals at some point that bring
2:16:34
you know rationality and judgment humility to it yeah those people are few and far between we should probably prize
2:16:39
them very highly yes celebrate humility in our public leaders uh so getting to
Misinformation
2:16:44
risk number two will AI ruin our society short version as you write if the murder
2:16:49
robots don't get us the hate speech and misinformation will and the action you recommend in short
2:16:56
don't let the thought police suppress AI well what is uh
2:17:03
this risk of the effect of misinformation of society that's going
2:17:09
to be catalyzed by AI yeah so this is the social media this is what you just alluded to it's the activism kind of
2:17:15
thing that's popped up in these companies and in the industry and it's basically from my perspective it's basically part two of the war that
2:17:21
played out over social media over the last 10 years um because you probably remember social media 10 years ago was basically who
2:17:27
even wants this who wants who wants a photo of what your cat had for breakfast like this stuff is like silly interview and why can't these nerds like figure
2:17:34
out how to invent something like useful and powerful and then you know certain things happened in the political system
2:17:39
and then it's sort of the polarity on that discussion switched all the way to social media is like the worst most corrosive most terrible most awful
2:17:45
technology ever invented and it leads to you know terrible the wrong you know politicians and policies and politics
2:17:51
and like and all this stuff and and that that all got catalyzed into this very big kind of angry movement both inside
2:17:56
and outside the companies to kind of bring social media to to heal and that got focused in particularly on two
2:18:01
topics so-called hate speech and so-called misinformation um and that's been to Saga playing out for the last for the last decade and I
2:18:07
don't even really want to even argue the pros and cons of the sides just to observe that that's been like a huge fight and it's had you know big
2:18:12
consequences to how these companies operate um basically that same those same sets
2:18:18
of theories that same activist approach that same energy as being transplanted straight to Ai and you see that already
2:18:23
happening it's why you know GPT will answer let's say certain questions and not others um it's why it gives you the canned
2:18:29
speech about you know whenever it starts with as a large language model I cannot you know basically means that somebody has reached in there and told it it
2:18:35
can't talk about certain topics um do you think some of that is good so it's a it's an interesting question
2:18:41
um so a couple of couple observations um so so one is um the people who find this the most frustrating are the people who
2:18:46
are worried about the murder robots
2:18:52
people right they started with the term AI safety the term became AI alignment when the term became AI alignment is
2:18:58
when this switch happened from or where it is going to kill us all we're worried about hate speech and misinformation sure the AIX risk people have now
2:19:05
renamed their thing uh AI not kill everyone ISM um which I have to admit is a catchy
2:19:10
term and they are very frustrated by the fact that the sort of activist driven hate speech misinformation kind of thing
2:19:15
is taking over which is what's happened it's taken over the AI ethics field has been taken over by the hate speech misinformation people
2:19:21
um you know look would I like to live in a world in which like everybody was nice to each other all the time and nobody ever said anything mean and nobody ever
2:19:27
used a bad word and everything was always accurate and honest like that sounds great do I want to live in a world where there's like a centralized
2:19:33
thought police working through the tech companies to enforce the view of a small set of Elites that they're going to
2:19:38
determine what the rest of us think and feel like absolutely not they could be a middle ground somewhere like Wikipedia
2:19:44
type of moderation there's moderation of Wikipedia that is somehow crowdsourced where you
2:19:51
don't have centralized Elites uh but it's also not completely just a free-for-all because uh the if you have
2:19:58
the entirety of human knowledge at your fingertips you can do a lot of harm like
2:20:04
if you have a good assistant that's completely uncensored it can help you build a bomb they can help you
2:20:12
um mess with people's physical well-being right if they because that information is out
2:20:19
there on the internet and so they're presumably there's it would be
2:20:24
you could see the positives in um censoring some aspects of an AI model
2:20:29
when it's helping you commit literal violence yeah and there's a section later section of the essay where I talk
2:20:35
about bad people doing bad things yes right which which and and there's a there's a set of things that we should discuss there yeah um what happens in
2:20:41
practice is these line as you alluded to this already these lines are not easy to draw and what I've observed in the social media version of this is the way
2:20:48
I describe it as the slippery slope is not a fallacy it's an inevitability the minute you have this kind of activist personality that gets in a position to
2:20:55
make these decisions they they take it straight to Infinity like they they it it goes into the crazy Zone like almost
2:21:00
immediately and never comes back because people become drunk with power um right and they look if you're in the position to determine what the entire world
2:21:07
thinks and feels and reads and says like you're going to take it and you know Elon has you know ventilated this with the Twitter files
2:21:13
over the last you know three months and it's just like Crystal Clear like how bad it got there now yeah reason for optimism is what uh Elon is doing with
2:21:19
the community notes um um so Community knows is actually a very interesting thing uh so what Elon is
2:21:25
trying to do with Community notes um is he's trying to have it where there's only a community note when people who have previously disagreed on many topics
2:21:32
agree on this one yes that's interesting that's what that's what I'm trying to get at is like there's there could be Wikipedia like
2:21:38
models a community knows type of models where allows you to essentially either
2:21:44
provide context or censor in a way that's not resist the slippery slope nature now there's another Power there's
2:21:50
an entirely different approach here which is basically um we have AIS that are producing content we can also have AIS that are
2:21:56
consuming content yeah right and so one of the things that your assistant could do for you is help you consume all the
2:22:01
content right and basically tell you when you're getting played so for example I'm going to want the AI that my
2:22:06
kid uses right to be very you know child safe and I'm going to want it to filter for him all kinds of inappropriate stuff
2:22:11
that he shouldn't be saying just because he's a kid yeah right and you see what I'm saying is you can Implement that you could use the architectural you could
2:22:17
say you can solve this on the client side right solving on the server side gives you an opportunity to dictate for the entire world which I think is where
2:22:23
you you take the slippery slope to Hell there's another architectural approach which is to solve this on the client side which is certainly what I would
2:22:29
endorse it's uh at risk number five will AI lead uh to bad people doing bad things and I
2:22:35
can just imagine language models used to do so many bad things but the hope is there that you can have
2:22:41
uh large language models used to then defend against it by more people by smarter people by more effective people
2:22:48
skilled people all that kind of stuff three-point argument on bad people doing bad things um so um uh so number one
2:22:54
right you can use the technology defensively and there's a we should be using AI to build like broad spectrum vaccines and antibiotics for like
2:23:00
bioweapons and we should be using it to like hunt terrorists and catch criminals and like we should be doing like all kinds of stuff like that in fact we
2:23:06
should be doing those things even just to like go get like you know basically go eliminate risk from like regular pathogens that aren't like constructed
2:23:12
by an AI so there's there's the whole um uh there's a whole defensive set of things um second is we have many laws on
2:23:18
the books about the actual bad things right so it is actually illegal to be a you know to commit crimes to to commit
2:23:24
terrorist acts to you know build pathogens with the intent to deploy them to kill people and so we have those we
2:23:30
don't we actually don't need new laws for the vast majority of the scenarios we actually already have the laws in the book on the books the third argument is the
2:23:37
minute and this is sort of the foundational one that gets really tough but the minute you get into this thing which which you were kind of getting into which is like okay but like don't
2:23:44
you need censorship sometimes right and don't you need restrictions sometimes it's like okay what is the cost of that
2:23:50
um and in particular in the world of Open Source right um and so um is open source AI going to
2:23:55
be allowed or not um if open source AI is not allowed um then what is the regime that's going
2:24:02
to be necessary legally and technically to prevent it from developing right and here again is where you get
2:24:07
into and people have proposed these kinds of things you get into I would say pretty extreme territory pretty fast do we have a monitor agent on every CPU and
2:24:15
GPU that reports back to the government what we're doing with our computers are we seizing GPU clusters that get Beyond
2:24:21
a certain size like and then by the way how are we doing all that globally right and like if China is developing an llm
2:24:27
beyond the scale that we think is allowable are we going to invade right and you have figures on the AIX risk
2:24:33
side who are advocating and you know potentially up to nuclear strikes to prevent you know this kind of thing and so here you get into this thing and
2:24:39
again you know you could maybe say this is you know you could even say this is what good bad or indifferent or whatever but like here's the the comparison of
2:24:46
the nukes the comparison of nukes is very dangerous because one is just nukes for just just about although we can come back to nuclear power but the other
2:24:52
thing was like with nukes you could control plutonium right you could track plutonium and it was like hard to come by AI is just math and code right it's and
2:25:00
it's in like math textbooks and it's like their YouTube videos that teach you how to build it and like there's open source there's already open source you know there's a 40 billion parameter
2:25:06
model running around already called Falcon Online that anybody can download um and so okay you walk down the logic
2:25:13
path that says we need to have guardrails on this and you find yourself in a authoritarian totalitarian regime
2:25:18
of thought control and machine control that would be so brutal that you would
2:25:24
have destroyed the society that you're trying to protect and so I I just don't see how that actually works so yeah you
2:25:30
have to understand my brain has gone a full uh full steam ahead here because I agree with uh basically everything
2:25:36
you're saying when I'm trying to play devil's advocate here there because okay you've highlighted the fact that there
2:25:42
is a slippery slope to human nature the moment you censor something you start to censor everything
2:25:49
um the alignment starts out sounding nice but then you start align to uh the
2:25:57
beliefs of some select group of people and then it's just your beliefs the the number the
2:26:04
number of people you're lying to is smaller and smaller as that group becomes more and more powerful okay but
2:26:09
that just speaks to the people that censor are usually the and the get richer I wonder if it's
2:26:16
possible to do without that for AI the one way to ask this question
2:26:21
is do you think the base models the the base the Baseline Foundation model should be open sourced
2:26:28
like uh what where Mark Zuckerberg is saying they want to do so I look I mean
2:26:33
I think it's totally appropriate that companies that are in the business of producing a product or service should be
2:26:39
able to have a wide range of policies that they put right and now just again I want a heavily censored model for my
2:26:45
eight-year-old like I actually want that like like I would pay more money for the ones more heavily censored than the one that's not right
2:26:51
um and so like there are certainly scenarios where companies will make that decision look an interesting thing you
2:26:56
brought up the or is is this really a speech issue um one of the things that the big tech companies are dealing with
2:27:01
is that content generated uh from an llm is not covered under Section 230 uh
2:27:07
which is the law that uh protects internet platform companies from being sued for the user generated content
2:27:13
um and so it it's actually yes and so there's actually a there's actually a question I think there's still a
2:27:18
question which is can big can big American companies actually feel generative AI at all or is the liability
2:27:24
actually gonna just ultimately convince them that they can't do it because the minute the thing says some something bad
2:27:29
and it doesn't even need to be hate speech it could just be like an enact it could hallucinate a product you know detail on a vacuum cleaner you know and
2:27:36
all of a sudden the vacuum cleaner company sues for misrepresentation and there's any symmetry there right because the the LM is going to be producing
2:27:42
billions of answers to questions and it only needs to get a few wrongs the loss has to get updated really quick here yeah and nobody knows what to do with
2:27:48
that right um so anyway like they're they're a big they're big questions around how companies operate at all so
2:27:54
we talk about those but then there's this other question of like okay the open source so what about open source and my answer to your question is kind
2:28:00
of like obviously yes the models have there has to be full open source here because to live in a world in which that
2:28:06
open source is not allowed is a world of draconian speech control human control
2:28:11
machine control I mean you know black helicopters with Jack booted thugs coming out repelling down and seizing
2:28:17
your GPU like territory well no no I'm 100 serious that's you're saying
2:28:23
slippery slope always leads there no no no no no that's what's required to enforce it like how will you enforce a ban on open
2:28:28
you could add friction to it like harder to get the models because people will always be able to get the models but
2:28:34
it'll be more in the shadows right the leading opens first model right now is from the UAE it's like the next time
2:28:39
they do that what do we do yeah like oh I see you're
2:28:45
like uh the 14 year old in Indonesia comes out with a breakthrough model you know we talked about most great software
2:28:50
comes from a small number of people some kid comes out with some big new breakthrough in quantization or something and he has some huge breakthrough and like what we're gonna
2:28:56
what are we gonna like invade Indonesia and arrest him it seems like in terms of size and models and effectiveness of models the big tech
2:29:03
companies will probably lead the way for quite a few years and and the question is of what policies they should use the
2:29:10
the kid the kid in Indonesia should not be regulated but should Google meta
2:29:17
uh Microsoft open AI be regulated well so but this goes okay so when does it
2:29:23
become dangerous yeah right is is the danger that it's quote as powerful as the current leading
2:29:29
commercial model or is it that it is it is just at some other arbitrary threshold yeah and then by the way like
2:29:35
look how do we know like what we know today is that you need like a lot of money to like train these things but their advances being made every week on
2:29:41
training efficiency and you know data all kinds of synthetic you know look I don't even like the synthetic data thing we're talking about maybe some kid
2:29:46
figures out a way to Auto generate synthetically it's going to change everything yeah exactly and so like sitting here today like the the
2:29:52
Breakthrough just happened right you made this point like the Breakthrough just happened so we don't know what the
2:29:57
shape of this technology is going to be I mean the the big shock the the big shock here is that you know whatever
2:30:03
number of billions of parameters basically represents at least a very big percentage of human thought like who
2:30:09
would have imagined that and then there's already work underway there was just this paper that just came out that basically takes the gpt3 scale
2:30:16
model and compresses it down to run on a single 32 core CPU like who would have predicted that yeah
2:30:22
um you know some of these models now you can run a Raspberry Pi's like today they're very slow but like you know maybe they'll be a you know perform you
2:30:29
know like it's math and here we're back in here we're back math and code it's math and
2:30:35
codes math code and data it's bits marks just like blocked away at this point he's just screw it I don't know what to
2:30:42
do with this you guys created this whole internet thing yeah yeah I mean I'm a
2:30:48
huge believer in open source here so my argument is we're gonna have to see here's my argument my argument my full argument is AI is going to be like air
2:30:54
it's going to be everywhere like this is just going to be in Texas it already is it's going to be in textbooks and kids are going to grow up knowing how to do this and it's just going to be a thing
2:31:00
it's going to be in the air and you can't like pull this back anywhere you can pull back air and so you just have to figure out how to live in this world
2:31:05
right and then that and then that's where I think like all this hand ringing but air risk is basically complete waste of time because the the the effort
2:31:11
should go into okay what are what what is the defensive approach and so if you're worried about you know AI
2:31:17
generated pathogens the right thing to do is to have a permanent project warp speed right funded lavishly let's do it
2:31:22
Manhattan let's talk about let's do a Manhattan project for biological defense right and let's build AIS and let's have
2:31:27
like broad spectrum vaccines where like we're insulated from every pathogen well the interesting thing is because
2:31:34
it's software a kid in his basement teenager could build like a system that defends against
2:31:41
like the worst the the worst I mean and to me defense is super exciting
2:31:47
it's to like I If you believe in the good of human nature that most people
2:31:52
want to do good to be the savior of humanity is really exciting yes
2:31:58
okay that's a dramatic statement but like to help people to help people yeah okay what about just the jump
AI and the economy
2:32:04
around what about the risk of will AI lead to crippling inequality
2:32:09
you know because we're kind of saying everybody's life will become better is it possible that the the rich get
2:32:15
richer here yeah so this is actually ironically goes back to Marxism so um because this was the code so the court
2:32:20
claim of Marxism right basically was that the owner the owners of capital would basically own the means of production and then over time they would
2:32:25
basically accumulate all the wealth the workers would be paying in you know and getting nothing in return because they
2:32:31
wouldn't be needed anymore right marks was very worried about what he called mechanization or what later became known as automation
2:32:36
um and that you know the workers would be miserated and the capitalists would end up with with all and so this was one of the core core core principles of
2:32:42
Marxism of course it turned out to be wrong about every previous wave of Technology um the reason it turned out to be wrong
2:32:47
about every previous wave of technology is that the way that the self-interested owner of the machines makes the most
2:32:53
money is by providing the production capability in the form of products and services to the most people the most
2:32:59
customers as possible Right the largest this is one of those funny things where every CEO knows this intuitively and yet
2:33:05
it's like hard to explain from the outside the the way you make the most money in any business is by selling to the largest market you can possibly get
2:33:10
to the largest market you can possibly get to is everybody the planet and so every large company does is everything
2:33:16
that it can to drive down prices to be able to get volumes up to be able to get to everybody on the planet and that happened with everything from
2:33:22
electricity it happened with telephones it happened with radio it happened with automobiles it happened with smartphones it happened with the PCS it happened
2:33:31
with the internet it happened with mobile broadband it's happened by the way with Coca-Cola and it's happened
2:33:36
with like every you know basically every industrially produced you know good or service people you want to drive it to
2:33:42
the largest possible market and then as proof of that it's already happened right which is the early adopters of
2:33:48
like jgpt and Bing are not like you know Exxon and Boeing they're you know your
2:33:53
uncle and your nephew right it's just like free it's either freely available online or it's available for 20 bucks a
2:33:58
month or something but it you know these things went this this technology went Mass Market immediately
2:34:04
um and so look the the owners of the means of production whoever does this doesn't mention these trillion other questions there are people who are going
2:34:09
to get really rich doing this producing these things but they're going to get really rich by taking this technology to the broadest possible Market
2:34:15
so yes they'll get rich but they'll get rich having a huge positive impact on yeah making that making the technology
2:34:21
available to everybody yeah right and again smartphone same thing right so there's this amazing kind of twist in um
2:34:27
in business history which is you cannot spend ten thousand dollars on a smartphone right you can't spend a hundred thousand
2:34:32
dollars you can't spend like I would buy the million dollar smartphone like I'm signed up for it like if it's like suppose a million dollar smartphone was like much better than the thousand
2:34:39
dollar smartphone like I'm there to buy it it doesn't exist why doesn't it exist Apple makes so much more money driving
2:34:44
the price further down from a thousand dollars than they would trying to harvest right and so it's just this repeating pattern you see over and over
2:34:49
again um where the and and what's what's great about it what's great about it is you do not need to rely on anybody's
2:34:55
enlightened right generosity to do this you just need to rely on capitalist self-interest
2:35:00
uh what about AI taking our jobs yeah so very very similar thing here um
2:35:06
there's sort of a there's a core fallacy which again was was very common in Marxism which is What's called the lump of Labor fallacy and this is sort of the
2:35:12
fallacy that there's a only a fixed amount of work to be done in the world and if the and it's all being done today
2:35:17
by people and then if machines do it there's no other work to be done by people um and that's just a completely
2:35:22
backwards view on how the economy develops and grows um because what happens is not in fact that what happens
2:35:28
is the introduction of Technology into production process causes prices to fall as prices fall consumers have more
2:35:35
spending power is consumers have more spending power they create new demand that new demand then causes capital and
2:35:41
labor to form into new Enterprises to satisfy nuance and needs and the result is more jobs and higher wages the nuance
2:35:48
and needs the the worries that the the creation of new wants and needs at a rapid rate
2:35:54
will mean there's a lot of turnover in jobs so people will lose jobs just the
2:35:59
actual experience of losing a job and having to learn new things and your skills is painful for the individual
2:36:04
well two things one is the new jobs are often much better um so this actually came up as if there was this Panic about a decade ago and
2:36:10
all the truck drivers are going to lose their jobs right and number one that didn't happen because we haven't figured out a way to actually finish that yet
2:36:16
but but the other thing was like electric driver like I grew up in a town that was basically consisted of a truck stop right and I like knew a lot of
2:36:22
truck drivers and like truck drivers live a decade shorter than everybody else like they it's a it's a it's
2:36:27
actually like a very dangerous like they get like literally they have like high rates of skin cancer and on the left side of their on the left side of their
2:36:33
body from from being in the sun all the time the vibration of being in the truck is actually very damaging to your to your physiology and there's actually a
2:36:40
uh perhaps partially because of that reason uh there's a shortage yeah of uh
2:36:45
people who want to be truck drivers yeah like it's not it's not like the question always you want to ask somebody like that is do you want you know do you want
2:36:52
your kid to be doing this job and like most of them will tell you no like I want my kid to be sick in a cubicle somewhere like where they don't have
2:36:58
this like where they don't die 10 years earlier and so so the new jobs number one the new jobs are often better but
2:37:03
you don't get the new jobs until you go through the change and then to your point the the training thing you know it's always the issue is can can people
2:37:09
adapt and again here you need to imagine living in a world in which everybody has the AI assistant capability
2:37:14
right to be able to pick up new skills much more quickly and be able to have some you know be able to have a machine to work with to augment their skills it's still going to be painful but
2:37:21
that's the process of life it's painful for some people I mean there's no look there's no question it's painful for some people and they're you know they're
2:37:26
yes it's not again I'm not a utopian on this and it's not like it's positive for everybody in the moment but it has been
2:37:31
overwhelmingly positive for 300 years I mean look the concern here the concern the concern this concern has played out
2:37:37
for for literally centuries um and you know this is the sort of what I you know the story of the ludd eyes
2:37:43
um that you may remember there was a panic in the 2000s around uh Outsourcing who's going to take all the jobs there
2:37:48
was a panic in the 2010s that robots are going to take all the jobs um
2:37:53
in 2019 before covid we had more jobs at higher wages both in the country and in
2:37:58
the world than at any point human history and so the overwhelming evidence is that the net gain here is like just
2:38:04
like wildly positive and most most people like overwhelmingly come out the other side being huge beneficiaries of
China
2:38:10
this so you write that the single greatest risk this is the risk you're most convinced by the single greatest
2:38:17
risk of AI is that China wins Global AI dominance and we the United States and
2:38:22
the West do not can you elaborate yeah so this is the other thing which is a
2:38:28
lot of the sort of AI risk debates today sort of assume that we're the only game in town right and so we have the ability to kind of sit in the United States and
2:38:34
criticize ourselves and you know have our government like you know beat up on our companies and figure out a way to restrict what our companies can do and
2:38:39
you know we're gonna you know we're gonna ban this and ban that restrict this and do that and then there's this like other like force out there that
2:38:45
like doesn't believe we have any power over them whatsoever and they have no desire to sign up for whatever rules we
2:38:50
decide to put in place um and they're going to do whatever it is they're going to do and we have no control over it at all
2:38:55
and it's China and specifically the Chinese Communist Party um and they have a completely publicized
2:39:02
open you know uh plan for what they're going to do with AI and it is not what
2:39:07
we have in mind um and not only do they have that as a vision and a plan for their society but they also have it as a vision and plan
2:39:13
for the rest of the world so their plan is what surveillance yeah authoritarian control so authoritarian population
2:39:18
control um you know good old-fashioned communist authoritarian control um and surveillance and enforcement
2:39:26
um and social credit scores and all the rest of it um and you are going to be monitored and metered within an inch of everything all
2:39:32
the time um and it's you know basically the end of human freedom and that's their goal and you know they justify it on the
2:39:38
basis of that's what leads to peace and you're worried that the uh regulating in the United States will
2:39:45
will call progress enough to where uh the Chinese government would win that race so their plan yeah yes yes and the
2:39:52
reason for that is they and again they're very public on this they have their plan is to prolifer their their approach around the world and they have
2:39:57
this program called the digital Silk Road right which is building on their their Silk Road investment program and they've got their they've been laying
2:40:03
they've been laying networking infrastructure all over the world with their 5G right work with their company Huawei and so they've been laying all
2:40:09
this fabric but financial and technological fabric all over the world and their plan is to roll out their vision of AI on top of that and to have
2:40:15
every other country be running their version and then if you're a country prone to you know authoritarianism
2:40:21
you're going to find this to be an incredible way to become more authoritarian uh if you're a country by the way not prone authoritarianism
2:40:27
you're going to have the Chinese Communist Party running your infrastructure and having back doors into it right which is also not good
2:40:34
um what's your sense of where they stand in terms of the race towards super intelligence as compared to the United
2:40:40
States yeah so good news is they're behind but bad news is they you know they let's just say they get access to everything we do
2:40:46
um so they're probably a year behind at each point in time but they get you know downloads I think of basically all of
2:40:52
our work on a regular basis through a variety of means um and they are you know we'll see
2:40:57
they're at least putting out reports of very just put out a report last week of a gpg 3.5 analog
2:41:02
um they put out this report forget what it's called but um they put out this report of the cell and they did and they
2:41:07
you know the way when openai you know puts out they they one of the ways they test you know a GPT um is they they run
2:41:14
it through standardized exams like the sat right just how you can kind of cage how smart it is uh and so the Chinese
2:41:19
report they ran their LM through uh the Chinese equivalent of the SAT um and it includes a section on Marxism
2:41:27
um and a section on Mouse thought and it turns out their AI does very well on both of those topics
2:41:32
right so like uh this this alignment thing communist AI right like literal
2:41:39
communist AI right and so their vision is like that's the you know so you know you can just imagine like you're a
2:41:45
school you know you're a kid 10 years from now in Argentina or in Germany or in who knows where uh Indonesia and you
2:41:53
ask they I'd explain to you like how the economy works and it gives you're the most cheery upbeat explanation of Chinese style communism you've ever
2:41:59
heard right so like the stakes here are like really big well my as we've been talking about my
2:42:05
hope is not just with the United States but it would just uh the kit in his basement with open source llm because I
2:42:11
I don't know if I um trust large centralized institutions with super powerful Ai No matter what
2:42:18
their ideology is a power corrupts you've been investing in tech companies
Evolution of technology
2:42:25
for about let's say 20 years and uh about 15 of which was uh with Andreessen
2:42:30
Horowitz uh what interesting Trends in Tech have you seen over that time let's just talk
2:42:36
about companies and just the evolution of the tech industry I mean the big shift over 20 years has been that Tech
2:42:42
used to be a tools industry for basically from like 1940 through to about 2010 almost all the big successful
2:42:49
companies were picks and travels companies so PC database smartphone you know some some some tool that somebody
2:42:55
else would pick up and use since 2010 most of the big wins have been in applications
2:43:01
um so a company that starts a com uh you know it starts in an existing industry and goes directly to the customer in
2:43:07
that industry and you know the early examples there were like uber and Lyft and Airbnb
2:43:12
um and then that model is kind of elaborating out um the AI thing is actually a reversion
2:43:18
on that for now because like most of the AI business right now is actually in Cloud provision of of AI apis for other
2:43:23
people to build on but but the big thing will probably be an app yeah I think I think most of the money I think probably
2:43:29
will be in whatever yeah your AI financial advisor or your AI doctor or your AI lawyer or you know take your
2:43:35
pick of whatever the domain is um and there and what's interesting is you know we the valley kind of does everything we should our entrepreneurs
2:43:42
kind of elaborate every possible idea and so there will be a set of companies that like make AI something that can be
2:43:47
purchased and used by large law firms um and then there will be other companies that just go direct to Market as a as an
2:43:53
AI lawyer what advice could you give for a startup founder just haven't seen so many
2:44:00
successful companies so many companies that fail also what advice could you give to a startup founder someone who
2:44:06
wants to build the next super successful startup in the tech space the Googles the apples the twitters
2:44:14
yeah so the great thing about the really great Founders is they don't take any advice so
2:44:19
so if you find yourself listening to advice maybe you shouldn't do it um but that's actually just to elaborate
2:44:26
on that if you could also speak to Great Founders yeah like what makes a great
2:44:31
founder so what makes a great founder is super smart um coupled with super energetic coupled
2:44:37
with super courageous I think it's some of those those three and intelligence passion and courage the first two are
2:44:44
traits and the third one is a choice I think encourage is a choice well because courage is a question of pain tolerance
2:44:50
right um so um how how many times you're willing to get punched in the face before you
2:44:56
quit yeah um and here's maybe the biggest thing people don't understand about what it's like to be a startup founder is it gets very
2:45:03
romanticized right um and even when it even when they fail it still gets romanticized about like what a great adventure it was but like
2:45:09
the reality of it is most of what happens is people telling you no and then they usually follow that with you're stupid right no I will not come
2:45:16
to work for you um I will not leave my cushy job at Google can work for you no I'm not going to buy your products you know no I'm not going to run a story
2:45:22
about your company no I'm not this that the other thing um and so a huge amount of what people have to do is just get used to just
2:45:28
getting punched and and the reason people don't understand this is because when you're a Founder you cannot let on that this is happening because it will
2:45:33
cause people to think that you're weak and they'll lose faith in you yeah so you have to pretend that you're having a great time when you're dying inside
2:45:40
right it's just a misery but why why do they do it what do they do oh yeah that's the
2:45:46
thing it's it's like it is a level it's actually one of the conclusions I think is it I think it's actually for most of these people on a risk adjusted basis
2:45:53
it's probably an irrational act they could probably be more financially successful on average if they just got like a real job in a big company
2:45:59
um but there's you know some people just have an irrational need to do something new and build something for themselves
2:46:04
and you know some people just can't tolerate having bosses oh here's the fun thing is how do you reference check founders right so you call it you know
2:46:11
normally you reference check your time hiring somebody as you call the bosses if they're in you know and you find out if they were good employees and now
2:46:16
you're trying to reference check Steve Jobs right and it's like oh God he was terrible you know he was a terrible employee he never did what we told him
2:46:22
to do yeah so what's a good reference do you want
2:46:27
the previous boss to actually stay there they never did what you told them to do that might be a good thing well ideally
2:46:33
ideally what you want is I will go I would like to go to work for that person um he worked for me here and now I'd
2:46:39
like to work for him no unfortunately most people can't their egos can't can't handle that so they won't say that but
2:46:44
that that's the ideal what advice would you give to those folks in the space of intelligence passion and courage so I
2:46:51
think the other big thing is you see people sometimes who say I want to start a company and then they kind of work through the process of coming up with an
2:46:57
idea and generally those don't work as well as the case where somebody has the idea first and then they kind of realize
2:47:04
that there's an opportunity to build a company and then they just turn out to be the right kind of person to do that when you say idea do you mean yeah
2:47:10
what long-term big Vision or do you mean specifics of like product specific let's
2:47:15
say specific like specifically what yes specifics like what is because for the first five years you don't get to have Vision you just got to build something
2:47:21
people want and you've got to figure out a way to sell it to them right it's very practical or you never get to Big Vision so so the first the first part you have
2:47:28
an idea of a set of products or the first product that can actually make some money yeah like it's got a first product's got to work by which I mean
2:47:34
like it has to technically work but then it has to actually fit into the category in the customer's mind of something that they want and then and then by the way
2:47:40
the other part is they have to want to pay for it like somebody's got to pay the bills and so you've got to figure out how to price it and whether you can
2:47:45
actually extract the money yeah so usually it is much more predictable
2:47:51
success is never predictable but it's more predictable if you start with a great idea and then back into starting the company
2:47:57
um so this is what we did you know before we escape the Google guys had the Google search engine working at Stanford
2:48:03
um right um the um uh you know yeah actually there's tons of examples where they you know uh Pierre omadir had eBay
2:48:08
working before he left his previous job so I really love that idea of just having a thing a prototype that actually
2:48:14
works before you even begin to remotely scale yeah by the way it's also far easier to raise money right like the
2:48:20
ideal pitch that we receive is here's the thing that works would you like to invest in our company or not like that's so much easier than here's 30 slides
2:48:26
with a dream right um and then we have this concept called the dmas which are apology Serena Boston
2:48:33
came up with um when he was with us um so so so then there's this thing this goes to mythology which is um you know
2:48:39
there's a mythology of that kind of you know these these ideas um you know kind of arrive like magic or people kind of stumble into them it's
2:48:44
like eBay with the pest dispensers or something um the reality usually with the big
2:48:49
successes is that the founder has been chewing on the problem for five or ten years before they start the company and
2:48:56
they often worked on it in school um or they even experimented on it when they were a kid
2:49:01
um and they've been kind of training up over that period of time to be able to do the thing so they're like a true domain expert
2:49:07
and and it's it sort of sounds like Mom and apple pie which is yeah you want to be a domain expert what you're doing but you would you know the mythology is so
2:49:14
strong of like oh I just like had this idea in the shower and now I'm doing it like it's generally not that no because well maybe in the shower we had the
2:49:21
exact product implementation details but yeah
2:49:27
usually you're going to be for like years if not decades thinking about
2:49:32
like everything around that well we call it the idea Mas because the idma is
2:49:38
basically is like there's all these permutations like for any ID for any idea there's like all these different permutations who should the customer be
2:49:44
what shape forms the product have and how should we take it to Market and all these things um and so um the really smart Founders
2:49:51
have thought through all these scenarios by the time they go out to raise money um and they have like detailed answers
2:49:56
on every one of those fronts because they put so much thought into it um the sort of the the sort of more
2:50:02
haphazard Founders haven't thought about any of that and it's the detailed ones who tend to do much better so how do you
2:50:07
know when to take a leap if you have a cushy job or a happy life I mean the best reason is just because
2:50:13
you can't tolerate not doing it right like this is the kind of thing where if you have to be advised into doing it you probably shouldn't do it
2:50:19
um and so it's probably the opposite which is you just have such a burning sense of this has to be done I have to do this I have no choice what if it's
2:50:25
going to lead to a lot of pain it's going to lead to a lot of pain I think that's what if it means uh
2:50:31
losing sort of social relationships and damaging your um relationship with loved ones and all
2:50:37
that kind of stuff yeah look so like it's going to put you in a social tunnel for sure right so you're gonna like uh
2:50:42
you know there's this game you can play on Twitter which is you can do any whiff of the idea that there's uh basically
2:50:48
any such thing as work-life balance and that people should actually work hard and everybody gets mad but like the truth is like all the successful
2:50:54
Founders are working 80 hour weeks and they're working you know they perform very very strong social bonds with the
2:50:59
people they work with they tend to lose a lot of friends on the outside or put those friendships on ice like that's just the nature of the of the thing
2:51:06
um you know for most people it's worth the trade-off you know the advantage you know maybe younger Founders have is maybe they have less you know maybe
2:51:11
they're not you know for example if they're not married yet or don't have kids yet that's an easier thing to bite off can you be an older founder yeah you
2:51:17
definitely can yeah um yeah many of the most successful Founders are second third fourth time Founders they're in their 30s 40s 50s um the good news of
2:51:24
being an older founder is you know more and you you know a lot more about what to do which is very helpful the problem is okay now you've got like a spouse and
2:51:31
a family and kids and like you've got to go to the baseball game and like you can't go to the base you know and so it's it
2:51:36
life is full of difficult choices yes uh you've written a blog post on what
How to learn
2:51:43
you've been up to uh you wrote this in October 2022 I quote mostly I try to
2:51:48
learn a lot for example the political events of 2014 to 2016 made clear to me that I didn't
2:51:54
understand politics at all referencing maybe some of this this book here
2:51:59
um so I deliberately withdrew from political engagement and fundraising and instead read my way back into history
2:52:06
and as far to the political left and political right as I could so just high
2:52:11
level question what's your approach to learning yeah so it's basically I would say it's
2:52:16
it's an Auto Direct um uh so it's sort of goes it's going down the rabbit holes so it's a
2:52:22
combination let's say I kind of allude to it in that quote It's a combination of breadth and depth um and so I tend to yeah I tend to I go
2:52:29
abroad by the nature of what I do I go broad but then I tend to go deep in a rabbit hole for a while read everything I can and then come out of it and I
2:52:35
might I might not revisit that rabbit hole for you know another decade and in that blog post that I'd recommend people
2:52:41
go check out you actually list a bunch of different books that you recommend on different topics on the American left
2:52:46
and the American right uh it's just a lot of really good stuff the best explanation for the current
2:52:52
structure of our society in politics you give to recommendations four books on the Spanish Civil War six books on deep
2:52:58
history of the American right comprehensive biocracy disease of Adolf Hitler of one of which I read I can
2:53:04
recommend uh six books in the Deep history of the American left so American right American left looking at the
2:53:10
history to give you the context um biography of uh Vladimir Lenin two of
2:53:15
them on the French Revolution actually I have never read a bag fan then maybe that that would be useful everything's
2:53:22
been so Marx focused the Sebastian biography of London is extraordinary uh Victor Sebastian okay blow your mind
2:53:28
yeah so it's still useful incredible yeah it's incredible I actually think it's the single best book on the Soviet Union so that the perspective of Lenin
2:53:36
it might be the best way to look at the Soviet Union versus Stalin versus Marx versus very interesting so two books on Fascism
2:53:42
and anti-fascism uh by the same uh author uh Paul Godfrey uh brilliant book
2:53:49
on the nature of mass movements and Collective psychology the definitive work on intellectual life under totalitarianism the captive mind uh the
2:53:56
definitive worked on the Practical life under total terrorism uh there's a bunch there's a bunch and the single best book
2:54:03
first of all the list here is just incredible but you say the single best book I have found on who we are and how
2:54:10
we got here is the ancient city uh by Numa Dennis festel di kulangus I like it
2:54:17
uh what's uh what did you learn about who we are as a human civilization from that book yeah so this is a fascinating
2:54:23
book this one's free but it's a free by the way it's a it's a book in 1860s you can download it or you can buy print out prints of it but um it's uh it was this
2:54:30
guy who was a professor at the sorbond in the 1860s and he was apparently a savant on uh Antiquity on Greek and
2:54:37
Roman Antiquity um and the reason I say that is because his sources are 100 original Greek and Roman sources so he
2:54:44
wrote a basically a history of Western Civilization from on the order of four thousand years ago to basically the
2:54:49
present times entirely working on Fresh original Greek and Roman Roman sources
2:54:54
um and what he was specifically trying to do was he was trying to reconstruct from the stories of the grace of the Romans he was trying to reconstruct what
2:55:01
life in the west was like before the Greeks and the Romans which was in this in this in the civilization known as the the indo-europeans
2:55:08
um and the short answer is and this is sort of Circa 4 000 you know 2000 BC to
2:55:13
you know sort of 500 BC kind of that 1500 year stretch where civilization developed and his conclusion was
2:55:19
basically Cults um they were basically Cults and the civilization was organized into Cults
2:55:25
and the the intensity of the Cults was like a million fold beyond anything that we would recognize today like it was a
2:55:31
level of um all encompassing belief and uh an action around religion
2:55:39
um that was at a level of extremeness that we wouldn't even recognize it um uh and and so specifically he tells
2:55:44
the story of basically there were three levels of Cults there was the family cult the tribal cult and then the city
2:55:50
cult as as Society scaled up and then each call was a joint Cult of family
2:55:57
Gods which were ancestor gods and then nature gods and then you're bonding into a family A
2:56:04
Tribe or a city was based on your adherence to that religion um people uh who were not of your family
2:56:10
tribe City worshiped different gods which gave you not just the right with the responsibility to kill them on site
2:56:17
so they were serious about their Cults hardcore by the way shocking development I did not realize a zero concept of
2:56:24
individual rights like even even up through the Greeks and even in the Romans they didn't have the concept of individual rights like the idea of it as
2:56:30
an individual you have like some right it's just like nope right and you look back and you're just like wow that's just like crazily like fascist and a
2:56:37
degree that we wouldn't recognize today but it's like well they were living under extreme pressure for survival and
2:56:42
you and you know the theory goes you could not have people running around making claims to individual rights when you're just trying to get like your tribe through the winter right like you
2:56:48
need like hardcore command and control and so and and actually what if through Modern political lens those Cults were
2:56:54
basically both fascist and communist um they were fascists in terms of social control and then they were Communists in terms of economics
2:57:00
but you think that's fundamentally that like pull towards uh calls is within us
2:57:06
well so so my conclusion from this book so so so so the way we naturally think
2:57:12
about the world we live in today is like we basically have such an improved version of everything that came before
2:57:17
us right like we we have basically we've figured out all these things around morality and ethics and democracy and all these things and like they were
2:57:23
basically stupid and retrograde and were like smart sophisticated and we've improved all this um I I after reading
2:57:28
that book uh I I now believe in many ways the opposite which is no actually we are still running in that original
2:57:33
model we're just running in an incredibly diluted version of it so we're still running basically in Cults
2:57:39
it's just our Cults are at like a thousandth or a millionth the level of intensity right and so our so just to
2:57:44
take religions you know the modern experience of a Christian in our time even somebody who considers some a
2:57:50
devout Christian is just a shadow of the level of intensity of somebody who belong to a religion back in that period
2:57:55
and then by the way we have constr goes back to our AI discussion we we we then sort of endlessly create new calls like
2:58:02
we're trying to fill the void right and the void is a void of bonding okay living in their era like everybody
2:58:09
living today transporting that era would view it as just like completely intolerable in terms of like the loss of freedom and the level basically fascist
2:58:15
a control however every single person in that era and he really stresses this they knew exactly where they stood they
2:58:21
knew exactly where they belonged they knew exactly what their purpose was they know exactly what they needed to do every day they know exactly why they
2:58:26
were doing it they had total certainty about their place in the universe so the question of meaning the question of purpose was very distinctly clearly
2:58:33
defined for them absolutely overwhelmingly undisputably undeniably so as we turn
2:58:39
the volume down on the cultism yes we start to uh the search for meaning starts getting harder and harder yes
2:58:45
because we don't have that we're we're ungrounded we're we're uncentered and we all feel it right and that's why we
2:58:50
reach for you know it's why we still reach for religion it's why we reach for you know these people start to take on
2:58:55
you know let's say you know a faith and science maybe Beyond where they should put it uh you know and by the way like sports teams are like a you know they're
2:59:01
like a tiny little version of a cult and you know the you know Apple Keynotes are a tiny little version of a cult right
2:59:07
you know political you know yeah and there's called you know there's full blown calls on both sides of the political Spectrum right now right um
2:59:13
you know operating in plain but still not full-blown compared as to what it was compared to what it used to I mean we would today consider full-blown but
2:59:20
like yes they're they're at like I don't know a hundred thousandth or something of the intensity of what people had back then so so we live in a world today that
2:59:26
in many ways is more advanced and moral and so forth and it's certainly a lot nicer much nicer world to live in but we live in a world that's like a very
2:59:32
washed out it's like everything has become very colorless and gray as compared to how people used to
2:59:37
experience things which is I think why we're so prone to reach for drama because we there's something in US
2:59:43
deeply evolved where we want that back and I wonder where it's all headed as we
2:59:49
turn the volume down more and more uh what advice would you give to Young Folks today uh in high school and college how to be
Advice for young people
2:59:56
successful in their career how to be successful in their life yeah so the tools that are available today I mean
3:00:01
are just like I sometimes you know I sometimes bore uh you know kids by describing like what it was like to go
3:00:07
look up a book you know to try to like discover a fact and you know in in the old days the 1970s 1980s go to the
3:00:12
library in the card catalog and the whole thing you go through all that work and then the book is checked out you have to wait two weeks and like like
3:00:18
to be in a world not only where you can get the answer to any question but also the world now you know the AI world
3:00:23
where you've got like the assistant that will help you do anything help you teach learn anything like your ability both to learn and also to produce is just like I
3:00:30
don't know a million fold beyond what it used to be I have a I have a blog post I've been wanting to write um I wish I
3:00:35
call uh where where are the hyper productive people um like the question right like with
3:00:42
these tools like there should be authors that are writing like hundreds or thousands of like outstanding books
3:00:47
well with the authors there's a consumption question too but yeah well maybe not maybe not you're right but so
3:00:54
the tools are much more powerful they're getting much more musicians yeah right why aren't musicians producing a
3:01:00
thousand times the number of songs right um like what like the tools are spectacular so what uh what's the
3:01:07
explanation and by way of advice like what is motivation starting to be turned
3:01:13
down a little bit or what I think it might be a distraction distraction it's it's so easy to just sit and consume
3:01:19
um that I think people get distracted from production but if you wanted to um you know as a young person if you
3:01:24
wanted to really stand out you could get on a like a a hyper productivity curve very early on
3:01:30
there's a great uh you know the story there's a great story in Roman history of Pliny the Elder who was this legendary Statesman
3:01:36
um died in the Vesuvius eruption trying to rescue his friends but um he was famous both for being a uh basically
3:01:42
being a polymath but also being an author and he wrote apparently like hundreds of books most of which have been lost but he like wrote all these
3:01:47
encyclopedias and he literally like would be reading and writing all day long no matter what else was going on
3:01:52
and he so he would like travel with like four slaves and two of them were responsible for reading to him and two of them are responsible for taking
3:01:57
dictation and so like he'd be going cross-country and like literally he would be writing books like all the time and apparently
3:02:04
they were spectacular there's only a few that have survived but apparently they were amazing so there's a lot of value to being somebody who finds focus in
3:02:10
this life yeah like one and there are examples like there are uh you know there's this guy uh judge uh what's his name Posner postner um who wrote like 40
3:02:16
books and was also a great federal judge um you know there's uh our friend biology I think is like this he's one of
3:02:22
these you know where his his output is just prodigious um and so it's like yeah I mean with these tools why not and I
3:02:28
kind of think we're we're at this interesting kind of freeze-free moment where like this these tools are on everybody's hands and everybody's just kind of staring at them trying to figure
3:02:34
out what to do yeah but the new tools we have discovered fire yeah and trying to figure out how to use it to cook yeah
Balance and happiness
3:02:41
uh you told Tim Ferriss that the perfect day is caffeine for 10 hours and alcohol for four hours
3:02:47
you didn't think I'd be mentioning this did you uh it balances everything out perfectly as you said uh so let me ask
3:02:55
what's what's the secret to balance and maybe to happiness in life um I I don't believe in Balance so I
3:03:02
when I'm with the wrong person can you elaborate why you don't believe in Balance I mean I I maybe it's just and I
3:03:07
I look I think people I think people are wired differently so I think it's hard to generalize uh this kind of thing but
3:03:12
I'm I'm much happier and more satisfied when I'm fully committed to something so I'm very much in favor of it of
3:03:18
imbalance yeah imbalance and that applies to work to life to everything
3:03:23
yeah no no I happen to have whatever Twist of personality traits lead that in non-destructive Dimensions including the
3:03:29
fact that I've actually I now no longer do the 10-4 plan I stopped drinking I do the caffeine but not the alcohol so
3:03:34
there's something in my personality where I I whatever maladaption I have is inclining me towards productive things
3:03:40
not unproductive things so you're one of the wealthiest people in the world what's the relationship between wealth
3:03:46
and happiness oh uh money and happiness so I think happiness
3:03:52
I don't think happiness is the thing to strive for I think satisfaction is the thing
3:03:58
that's that just sounds like happiness but turned down a bit no deeper so happiness is you know a walk in the
3:04:05
woods at Sunset an ice cream cone a kiss um the first ice cream cone is great the
3:04:12
thousandth ice cream cone not so much at some point the walks of the woods get boring what's the distinction between happiness and uh
3:04:19
satisfaction satisfaction is a deeper thing which is like having found a purpose and fulfilling it being useful
3:04:26
so just uh something that permeates all your days just this General contentment
3:04:31
of of being useful then I'm fully satisfying my faculties that I'm fully
3:04:37
delivering right on the gifts that I've been given that I'm you know not making the world better that I'm contributing
3:04:43
to the people around me right and that I can look back and say wow that was hard but it was worth it
3:04:48
I think generally seems to lead people in a better State than Pursuit pleasure pursuit of quote unquote happiness does
3:04:55
money have anything to do with that I think the founders of the founding fathers in the US threw this off kilter when they use the phrase Pursuit of
3:05:00
Happiness I think they should have said they said pursuit of satisfaction we might live in a better world today well
3:05:06
you know they could have elaborated in a lot of things they could have tweaked the Second Amendment I think they were smarter than they realized they said you
3:05:13
know what we're going to make it ambiguous and let these uh these humans figure out the rest these tribal cult-like humans figure out the rest
3:05:21
uh but money empowers that so I I think and I think there I mean look I think Elon
3:05:28
is I don't think I'm even a great example but I think Elon would be the great example of this which is like you know look he's a guy who from every every day of his life from the day he
3:05:34
started making money at all he just plows into the into the next thing um and so I think I think money is
3:05:39
definitely an enabler for satisfaction it was supposed to say money applied to happiness leads people down very dark paths
3:05:46
very destructive Avenues uh money applied to satisfaction I think could be it is a real tool
3:05:52
um I always by the way I was like you know Elon is the case study for Behavior but the other thing that it's always
3:05:57
really made me think is Larry Larry Page was asked one time what his approach to philanthropy was and he said oh I'm just my philanthropic plans just give all the
3:06:03
money to Elon right uh well let me actually ask you
3:06:09
about Elon what what are your um you've interacted with quite a lot of successful engineers and business people
3:06:15
what do you think is special about Elon we talked about Steve Jobs what uh
3:06:20
what do you think is special about him as a leader as an innovator yeah so the the core of it is he's a he's he's back
3:06:27
to the future so he he is he is doing the most Leading Edge things in the world but with a with a really deeply
3:06:32
old school approach um and so to find comparisons to Elon you need to go to like Henry Ford and
3:06:37
Thomas Watson and Howard Hughes and Andrew Carnegie right um Leila Stanford
3:06:43
um John D Rockefeller right you need to go to the what we're called the Bourgeois capitalist it's like the hardcore business owner operators who
3:06:50
basically built you know basically built industrialized Society um Vanderbilt um and it's a level of
3:06:58
Hands-On commitments um and uh depth um in the business
3:07:06
um coupled with an absolute priority uh towards truth um and towards
3:07:12
um how to put Science and Technology uh time to First principles that is just like absolutely it was just like
3:07:18
unbelievably absolute he really is ideal that he's only ever talking to Engineers like he does not tolerate yes
3:07:25
let's anybody I've ever met um he wants ground truth on every single topic
3:07:31
um and he runs his businesses directly day to day devoted to getting to ground truth in every single topic so uh
3:07:38
you think it was a good decision for him to buy Twitter I have developed a view in life did not second guess Elon Musk
3:07:45
I know this is gonna suck Craig crazy and unfounded but well I mean uh
3:07:51
he's got a quite a track record I mean look the car was a crazy I mean the car was I mean look she's done a lot of
3:07:56
things that seem crazy starting a new car company in the United States of America the last time somebody really tried to do that was the 1950s and it
3:08:02
was called Tucker automotive and it was such a disaster they made a movie about what a disaster it was
3:08:07
um and then Rockets like who does that like that's there's obviously no way to start a rocket company like those days
3:08:13
are over and then to do those at the same time so after he pulled those two off like
3:08:18
okay fine like like this is one of my areas of like whatever opinions I had about that is just like okay clearly
3:08:24
they're not relevant like this is you just you at some point you just like bet on the person and in general I wish more people would lean on celebrating and
3:08:31
supporting versus deriding and destroying oh yeah I mean look he drives resentment like it's like he is a magnet
3:08:39
for resentment um like his critics are the most miserable like resentful people in the
3:08:44
world like it's almost a perfect match of like the most idealized you know technologists you know of the century
3:08:51
coupled with like just his critics are just bitter as can be I mean it's it's I mean it's it's sort of very Darkly uh
3:08:57
comic to watch well he uh he fuels the fire of that by being an on Twitter at times and
3:09:04
which is fascinating to watch the drama of human civilization given our cult roots
3:09:10
just fully on fire he's running a cult say that very successfully so now now
Meaning of life
3:09:17
there are Cults have gone and we search for meaning what do you think is the meaning of this whole thing what's the
3:09:22
meaning of life Mark kind reason I don't know the answer to that um I think the meaning of uh of uh the closest I get to
3:09:30
it is what I said about satisfaction so it's basically like okay we were given what we have like we should basically do our best what's the role of love in that
3:09:37
mix I mean like what's the point of life if you're yeah without love like yeah so love is a big part of that
3:09:43
satisfaction but really they look like they're taking care of people is like a wonderful thing like you know a mentality you know there are
3:09:49
pathological forms of taking care of people but there's also a very fundamental you know kind of aspect of taking care of people like for example I
3:09:55
happen to be somebody who believes the capitalism and taking care of people are actually they're actually the same thing
3:10:00
um somebody once said capitalism is how you take care of people you don't know right um right and so like yeah I think it's
3:10:07
like deeply woven into the whole thing um you know there's a long conversation we had about that but yeah
3:10:12
yeah creating products that are used by millions of people and bring them joy in smaller big ways and then capitalism
3:10:18
kind of enables that encourages that David Friedman says there's only three ways to get somebody to do something for
3:10:25
somebody else love money and force
3:10:31
love and money are better yeah that's a good ordering I think we appreciate it
3:10:36
we should bet on those try love first if that doesn't work the money yes and then Force well don't even try that one uh
3:10:43
mark You're an incredible person I've been a huge fan I'm glad to finally got a chance to talk I'm a fan of everything
3:10:48
you do everything you do including on Twitter it's a huge honor to meet you to talk with you uh thanks again for doing
3:10:54
this awesome thank you Lex thanks for listening to this conversation with Mark Andreessen to
3:10:59
support this podcast please check out our sponsors in the description and now let me leave you with some words Mark
3:11:05
Andreessen himself the world is a very malleable place if you know what you want and you go for
3:11:12
it with maximum energy and drive and passion the world will often reconfigure itself
3:11:18
around you much more quickly and easily than you would think thank you for listening and hope to see